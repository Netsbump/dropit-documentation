---
title: AccÃ¨s aux donnÃ©es
description: ImplÃ©mentation de la couche d'accÃ¨s aux donnÃ©es avec MikroORM
---

## Introduction

Cette section dÃ©taille l'implÃ©mentation concrÃ¨te de la couche d'accÃ¨s aux donnÃ©es dans DropIt. AprÃ¨s avoir prÃ©sentÃ© l'[architecture logicielle](/conception/architecture) d'ensemble et Ã©tabli le [modÃ¨le de donnÃ©es](/conception/base-donnees), nous explorons ici comment transformer ces concepts en code fonctionnel avec MikroORM et l'architecture hexagonale que j'ai adoptÃ©e dans l'API NestJS.

## Approches d'implÃ©mentation de la couche de donnÃ©es

AprÃ¨s avoir Ã©tabli le modÃ¨le conceptuel avec la mÃ©thode Merise, plusieurs approches s'offraient Ã  moi pour implÃ©menter la couche d'accÃ¨s aux donnÃ©es dans DropIt. Chacune prÃ©sente des avantages selon le contexte de dÃ©veloppement et les contraintes techniques du projet.

### Database First

Cette approche aurait consistÃ© Ã  crÃ©er directement les tables PostgreSQL via des scripts SQL, puis gÃ©nÃ©rer les entitÃ©s TypeScript Ã  partir du schÃ©ma existant. Pour illustrer cette mÃ©thode, voici comment j'aurais pu crÃ©er la table `workout` :

```sql
CREATE TABLE workout (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    category_id UUID NOT NULL,
    created_by UUID,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    FOREIGN KEY (category_id) REFERENCES workout_category(id),
    FOREIGN KEY (created_by) REFERENCES users(id)
);

-- Table de jointure polymorphe pour les Ã©lÃ©ments de workout
CREATE TABLE workout_element (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    workout_id UUID NOT NULL,
    type VARCHAR(20) NOT NULL CHECK (type IN ('exercise', 'complex')),
    exercise_id UUID,
    complex_id UUID,
    order_position INTEGER NOT NULL,
    sets INTEGER DEFAULT 1,
    reps INTEGER DEFAULT 1,
    rest INTEGER,
    start_weight_percent DECIMAL(5,2),
    FOREIGN KEY (workout_id) REFERENCES workout(id) ON DELETE CASCADE,
    FOREIGN KEY (exercise_id) REFERENCES exercise(id),
    FOREIGN KEY (complex_id) REFERENCES complex(id),
    CONSTRAINT check_one_element_type CHECK (
        (type = 'exercise' AND exercise_id IS NOT NULL AND complex_id IS NULL) OR
        (type = 'complex' AND complex_id IS NOT NULL AND exercise_id IS NULL)
    )
);
```

Cette mÃ©thode traditionnelle offre un contrÃ´le total sur la structure de la base de donnÃ©es et garantit des performances optimales grÃ¢ce Ã  la maÃ®trise fine des index et contraintes. Cependant, elle prÃ©sente plusieurs inconvÃ©nients.

La synchronisation entre le schÃ©ma de base de donnÃ©es et le code applicatif devient rapidement problÃ©matique. Chaque modification de table nÃ©cessite une mise Ã  jour manuelle de l'interface TypeScript correspondante. Si j'ajoute une colonne `difficulty_level` Ã  la table `workout`, je dois manuellement crÃ©er la propriÃ©tÃ© dans l'interface `WorkoutEntity`, avec le risque d'oublier cette Ã©tape ou de mal typer la propriÃ©tÃ©.

La maintenance des migrations peut reprÃ©senter Ã©galement un dÃ©fi considÃ©rable. Les Ã©volutions de schÃ©ma doivent Ãªtre gÃ©rÃ©es via des scripts SQL Ã©crits Ã  la main, avec toute la complexitÃ© que cela implique. Par exemple, l'ajout d'une contrainte NOT NULL sur une colonne existante nÃ©cessite de vÃ©rifier que toutes les donnÃ©es respectent cette contrainte, nettoyer les valeurs nulles existantes, puis appliquer la contrainte - le tout en prÃ©servant la cohÃ©rence des donnÃ©es pendant la migration.

Sans outil d'orchestration appropriÃ©, les diffÃ©rents environnements peuvent facilement diverger. Un dÃ©veloppeur qui applique manuellement un script SQL sur sa base locale mais oublie de le commiter dans le systÃ¨me de versioning crÃ©e une divergence silencieuse qui ne se rÃ©vÃ¨le qu'au moment du dÃ©ploiement en production.

### Schema First

Une approche intermÃ©diaire utilise un fichier de dÃ©finition de schÃ©ma central pour gÃ©nÃ©rer Ã  la fois la base de donnÃ©es et les entitÃ©s TypeScript. Cette mÃ©thode dÃ©finit la structure des donnÃ©es dans un format dÃ©claratif neutre, puis gÃ©nÃ¨re automatiquement les scripts SQL et les classes d'entitÃ©s correspondantes.

Le principe consiste Ã  maintenir une source de vÃ©ritÃ© unique qui dÃ©crit les tables, relations et contraintes, Ã©vitant ainsi les dÃ©synchronisations entre la base de donnÃ©es et le code applicatif. Cette solution rÃ©sout effectivement les problÃ¨mes de cohÃ©rence de l'approche Database First.

Cependant, cette approche ne s'harmonise pas idÃ©alement avec l'architecture monorepo que j'ai mise en place. Les packages partagÃ©s comme @dropit/schemas dÃ©finissent dÃ©jÃ  des schÃ©mas Zod rÃ©utilisÃ©s Ã  travers toutes les applications. Ajouter un fichier de schÃ©ma supplÃ©mentaire crÃ©erait une redondance : j'aurais les schÃ©mas Zod pour la validation cÃ´tÃ© client et serveur, plus un schÃ©ma sÃ©parÃ© pour la gÃ©nÃ©ration de base de donnÃ©es. Cette duplication augmente le risque d'incohÃ©rences et complique la maintenance.

De plus, l'intÃ©gration native avec l'Ã©cosystÃ¨me TypeScript se trouve compromise. Les types gÃ©nÃ©rÃ©s depuis un schÃ©ma externe ne bÃ©nÃ©ficient pas de l'auto-complÃ©tion et de la vÃ©rification de types aussi efficacement que les entitÃ©s dÃ©finies directement en TypeScript, et le partage de ces types avec les packages communs du monorepo devient plus complexe.

### Code First

J'ai donc finalement retenu l'approche Code First qui dÃ©finit les entitÃ©s directement en TypeScript avec les dÃ©corateurs MikroORM. Cette mÃ©thode rÃ©sout les inconvÃ©nients des deux approches prÃ©cÃ©dentes en s'intÃ©grant nativement dans l'Ã©cosystÃ¨me du monorepo. Les entitÃ©s MikroORM peuvent directement rÃ©utiliser et rÃ©fÃ©rencer les types dÃ©finis dans les packages partagÃ©s (@dropit/schemas, @dropit/contract), crÃ©ant une cohÃ©rence technique complÃ¨te.

La gÃ©nÃ©ration automatique des migrations Ã  partir des modifications d'entitÃ©s Ã©limine les risques de dÃ©synchronisation tout en prÃ©servant un contrÃ´le prÃ©cis sur la structure de donnÃ©es. Cette approche tire Ã©galement parti de l'auto-complÃ©tion et de la vÃ©rification de types native de TypeScript, facilitant le dÃ©veloppement et rÃ©duisant les erreurs de compilation.

## DÃ©finition des entitÃ©s MikroORM

AprÃ¨s avoir justifiÃ© le choix de l'approche Code First, il convient maintenant d'examiner concrÃ¨tement comment les entitÃ©s MikroORM traduisent le modÃ¨le conceptuel en implÃ©mentation TypeScript.

Les entitÃ©s constituent la traduction directe du modÃ¨le logique de donnÃ©es en classes TypeScript annotÃ©es. Chaque entitÃ© encapsule Ã  la fois la structure des donnÃ©es et les relations mÃ©tier.

### Structure type d'une entitÃ©

```typescript
@Entity()
export class Workout {
  @PrimaryKey({ type: 'uuid', defaultRaw: 'gen_random_uuid()' })
  id!: string;

  @Property()
  title!: string;

  @Property()
  description!: string;

  @ManyToOne(() => WorkoutCategory)
  category!: WorkoutCategory;

  @ManyToOne(() => User, { nullable: true })
  createdBy!: User | null;

  @OneToMany(() => WorkoutElement, (element) => element.workout)
  elements = new Collection<WorkoutElement>(this);

  @Property({ onCreate: () => new Date() })
  createdAt: Date = new Date();

  @Property({ onUpdate: () => new Date() })
  updatedAt: Date = new Date();
}
```

Cette entitÃ© `Workout` illustre plusieurs patterns que j'ai adoptÃ©s systÃ©matiquement :

**Identifiants UUID** : L'utilisation de `gen_random_uuid()` Ã©vite les conflits lors des synchronisations entre environnements et facilite les opÃ©rations de fusion de donnÃ©es.

**Relations typÃ©es** : Les dÃ©corateurs `@ManyToOne` et `@OneToMany` Ã©tablissent les relations avec typage strict, permettant Ã  TypeScript de dÃ©tecter les erreurs de navigation d'objets Ã  la compilation.

**Collections MikroORM** : Les relations one-to-many utilisent le type `Collection<T>` qui encapsule la logique de chargement paresseux et de gestion des relations bidirectionnelles.

**Timestamps automatiques** : Les propriÃ©tÃ©s `createdAt` et `updatedAt` s'actualisent automatiquement via les callbacks `onCreate` et `onUpdate`.

### Gestion des relations polymorphes

L'entitÃ© `WorkoutElement` illustre la rÃ©solution du pattern polymorphe identifiÃ© dans le modÃ¨le logique :

```typescript
@Entity()
@Check({
  name: 'check_one_element_type',
  expression: `
    (type = 'exercise' AND exercise_id IS NOT NULL AND complex_id IS NULL) OR
    (type = 'complex' AND complex_id IS NOT NULL AND exercise_id IS NULL)
  `,
})
export class WorkoutElement {
  @Enum({ items: () => Object.values(WORKOUT_ELEMENT_TYPES) })
  type!: WorkoutElementType;

  @ManyToOne(() => Exercise, { nullable: true })
  exercise?: Exercise;

  @ManyToOne(() => Complex, { nullable: true })
  complex?: Complex;

  @Property()
  sets!: number;

  @Property()
  reps!: number;
  
  // Autres propriÃ©tÃ©s communes...
}
```

Le dÃ©corateur `@Check` traduit la contrainte logique en contrainte PostgreSQL, garantissant l'intÃ©gritÃ© des donnÃ©es mÃªme en cas d'accÃ¨s direct Ã  la base. Cette approche combine la flexibilitÃ© du polymorphisme avec la rigueur des contraintes relationnelles.

Pour optimiser les performances futures des requÃªtes sur cette table polymorphe, un index composite sur `(type, exercise_id, complex_id)` pourrait s'avÃ©rer nÃ©cessaire selon l'Ã©volution des volumes. Actuellement, PostgreSQL utilise les index automatiques des clÃ©s Ã©trangÃ¨res, mais si les requÃªtes de filtrage par type deviennent frÃ©quentes, cet index spÃ©cialisÃ© accÃ©lÃ©rerait significativement les recherches d'Ã©lÃ©ments par discriminant.

## Architecture en couches et pattern Repository

Les entitÃ©s MikroORM dÃ©finissent la structure des donnÃ©es, mais leur utilisation dans l'application nÃ©cessite une architecture bien organisÃ©e pour sÃ©parer les responsabilitÃ©s et faciliter la maintenance. L'accÃ¨s aux donnÃ©es dans DropIt respecte une sÃ©paration stricte des responsabilitÃ©s via le pattern Repository et l'architecture hexagonale adoptÃ©e dans l'API NestJS.

### SÃ©paration des responsabilitÃ©s

L'architecture que j'ai mise en place respecte une sÃ©paration stricte des responsabilitÃ©s Ã  travers plusieurs couches distinctes. Chaque composant a un rÃ´le prÃ©cis que je vais dÃ©tailler avec des exemples concrets de l'implÃ©mentation DropIt.

Chaque module respecte une sÃ©paration stricte en quatre couches distinctes :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸŒ Interface Layer                        â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Controllers RESTâ”‚ â”‚ Guards &        â”‚ â”‚ DTOs &          â”‚â”‚
â”‚  â”‚                 â”‚ â”‚ Middlewares     â”‚ â”‚ Validators      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ğŸ“‹ Application Layer                      â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚ Use Cases       â”‚           â”‚ Services        â”‚          â”‚
â”‚  â”‚                 â”‚           â”‚ Applicatifs     â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ğŸ’ Domain Layer                        â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ EntitÃ©s MÃ©tier  â”‚ â”‚ RÃ¨gles Business â”‚ â”‚ Ports/Interfacesâ”‚â”‚
â”‚  â”‚                 â”‚ â”‚                 â”‚ â”‚                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   ğŸ”§ Infrastructure Layer                   â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ Repositories    â”‚ â”‚ Services        â”‚ â”‚ Adaptateurs     â”‚â”‚
â”‚  â”‚ MikroORM        â”‚ â”‚ Externes        â”‚ â”‚                 â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

Les ports dÃ©finissent les contrats d'interface nÃ©cessaires aux repositories et services externes, tandis que les adaptateurs fournissent les implÃ©mentations concrÃ¨tes correspondantes. Cette approche me donne la flexibilitÃ© de changer d'ORM, de base de donnÃ©es ou de services externes sans impacter la logique mÃ©tier centrale.

#### Interface Layer : exposition HTTP

**Controllers** gÃ¨rent uniquement le protocole HTTP et orchestrent les vÃ©rifications de sÃ©curitÃ© avant de dÃ©lÃ©guer la logique mÃ©tier. Ils remplissent plusieurs rÃ´les cruciaux :

```typescript
@UseGuards(PermissionsGuard) // 1. Garde globale sur toutes les mÃ©thodes
@Controller()
export class WorkoutController {
  
  @TsRestHandler(c.getWorkout) // 2. Respect du contrat ts-rest
  @RequirePermissions('read')  // 3. Permission spÃ©cifique requise
  getWorkout(
    @CurrentOrganization() organizationId: string, // 4. Extraction contexte organisation
    @CurrentUser() user: AuthenticatedUser         // 5. Extraction utilisateur authentifiÃ©
  ): ReturnType<typeof tsRestHandler<typeof c.getWorkout>> {
    return tsRestHandler(c.getWorkout, async ({ params }) => {
      // 6. DÃ©lÃ©gation immÃ©diate vers la logique mÃ©tier
      return await this.workoutUseCases.getWorkoutWithDetails(params.id, organizationId, user.id);
    });
  }

  @TsRestHandler(c.createWorkout)
  @RequirePermissions('create') // Permission diffÃ©rente pour la crÃ©ation
  createWorkout(
    @CurrentOrganization() organizationId: string,
    @CurrentUser() user: AuthenticatedUser
  ): ReturnType<typeof tsRestHandler<typeof c.createWorkout>> {
    return tsRestHandler(c.createWorkout, async ({ body }) => {
      return await this.workoutUseCases.createWorkout(body, organizationId, user.id);
    });
  }
}
```

Le controller orchestre plusieurs mÃ©canismes de sÃ©curitÃ© en cascade :

**Niveau 1 - Authentification** : Le `PermissionsGuard` vÃ©rifie que l'utilisateur possÃ¨de un token valide et extrait ses informations via `@CurrentUser()`. L'implÃ©mentation de ce systÃ¨me d'authentification est dÃ©taillÃ©e dans la section [authentification](/securite/authentification).

**Niveau 2 - Isolation organisationnelle** : Le dÃ©corateur `@CurrentOrganization()` garantit que l'utilisateur ne peut accÃ©der qu'aux ressources de son organisation, empÃªchant tout accÃ¨s transversal entre clubs.

**Niveau 3 - Permissions granulaires** : `@RequirePermissions('read')` vÃ©rifie que l'utilisateur dispose du droit spÃ©cifique requis pour cette action. Un membre simple peut avoir le droit 'read' mais pas 'create' ou 'delete'. Ce systÃ¨me de permissions est dÃ©taillÃ© dans la section [gestion des autorisations](/securite/permissions).

**Niveau 4 - Contrat d'API** : `@TsRestHandler(c.getWorkout)` assure que les paramÃ¨tres d'entrÃ©e et les rÃ©ponses correspondent exactement au contrat dÃ©fini dans `@dropit/contract`, garantissant la type safety entre l'API et les clients.

Cette approche multicouche me permet d'appliquer le principe de dÃ©fense en profondeur : mÃªme si une vÃ©rification Ã©choue, les autres barriÃ¨res protÃ¨gent l'accÃ¨s aux donnÃ©es. Le controller reste simple et focalisÃ© sur son rÃ´le d'orchestration HTTP, sans jamais contenir de logique mÃ©tier.

**Mappers** transforment les entitÃ©s de base de donnÃ©es en objets de transfert (DTO) pour l'API. Ils remplissent deux rÃ´les essentiels :

```typescript
export const WorkoutMapper = {
  toDto(workout: Workout): WorkoutDto {
    return {
      id: workout.id,
      title: workout.title,
      workoutCategory: workout.category.name, // Simplification : juste le nom au lieu de l'objet complet
      description: workout.description,
      elements: workout.elements.getItems().map(/* transformation des Ã©lÃ©ments */),
    };
  }
}
```

D'une part, ils **respectent le contrat d'API ts-rest** dÃ©fini dans `@dropit/contract`. Le type de retour `WorkoutDto` correspond exactement au schÃ©ma attendu par les clients, garantissant la cohÃ©rence entre l'API et les applications web/mobile qui l'utilisent.

D'autre part, ils **protÃ¨gent le schÃ©ma de base de donnÃ©es** en ne exposant pas directement les structures internes. Par exemple, l'entitÃ© `Workout` contient une relation complÃ¨te vers `WorkoutCategory` avec tous ses champs (id, description, createdBy, timestamps), mais le mapper ne expose que le nom de la catÃ©gorie. Cette approche Ã©vite de rÃ©vÃ©ler des dÃ©tails d'implÃ©mentation comme les clÃ©s Ã©trangÃ¨res, les champs techniques ou les relations qui ne concernent pas le client.

**Presenters** standardisent le formatage des rÃ©ponses et gÃ¨rent la logique de prÃ©sentation des donnÃ©es. Ils remplissent plusieurs responsabilitÃ©s importantes :

```typescript
export const WorkoutPresenter = {
  // SuccÃ¨s avec donnÃ©es
  presentOne(workout: WorkoutDto) {
    return { status: 200 as const, body: workout };
  },
  
  // SuccÃ¨s avec liste
  presentList(workouts: WorkoutDto[]) {
    return { 
      status: 200 as const, 
      body: workouts 
    };
  },

  // SuccÃ¨s de crÃ©ation (code diffÃ©rent)
  presentCreationSuccess(message: string) {
    return { 
      status: 201 as const, 
      body: { message } 
    };
  },

  // Gestion centralisÃ©e des erreurs
  presentError(error: Error) {
    if (error instanceof BadRequestException) {
      return { status: 400 as const, body: { message: error.message } };
    }
    if (error instanceof ForbiddenException) {
      return { status: 403 as const, body: { message: error.message } };
    }
    if (error instanceof NotFoundException) {
      return { status: 404 as const, body: { message: error.message } };
    }
    
    // Masquage des erreurs internes en production
    console.error('Workout error:', error);
    return {
      status: 500 as const,
      body: { message: 'An error occurred while processing the request' }
    };
  }
}
```

Le Presenter joue un rÃ´le crucial dans plusieurs aspects :

**Normalisation des codes de statut** : Il garantit que chaque type d'opÃ©ration retourne le code HTTP appropriÃ© (200 pour lecture, 201 pour crÃ©ation, 404 pour non trouvÃ©). Cette cohÃ©rence facilite la gestion cÃ´tÃ© client.

**SÃ©curisation des messages d'erreur** : Le Presenter filtre les erreurs techniques internes pour ne pas exposer de dÃ©tails d'implÃ©mentation au client. Une erreur de base de donnÃ©es devient un message gÃ©nÃ©rique, protÃ©geant la sÃ©curitÃ© de l'application.

**Centralisation du formatage** : Tous les Use Cases utilisent le mÃªme Presenter, garantissant un format de rÃ©ponse cohÃ©rent pour l'ensemble de l'API. Si je dÃ©cide de changer la structure des rÃ©ponses (ajouter des mÃ©tadonnÃ©es, modifier l'enveloppe JSON), un seul point de modification suffit.

**Ã‰volutivitÃ© du format de sortie** : Le Presenter pourrait facilement Ãªtre adaptÃ© pour produire d'autres formats que JSON : XML, CSV, ou mÃªme des templates HTML pour une interface web. Cette flexibilitÃ© ne nÃ©cessiterait aucune modification des Use Cases.


#### Application Layer : orchestration mÃ©tier

**Use Cases** concentrent la logique applicative et les rÃ¨gles mÃ©tier spÃ©cifiques au domaine de l'haltÃ©rophilie. Ils orchestrent les diffÃ©rents repositories tout en appliquant des vÃ©rifications mÃ©tier critiques pour la sÃ©curitÃ© des utilisateurs :

```typescript
async createWorkout(workout: CreateWorkout, organizationId: string, userId: string) {
  // 1. VÃ©rifications d'autorisation mÃ©tier
  const isCoach = await this.memberUseCases.isUserCoachInOrganization(userId, organizationId);
  if (!isCoach) throw new ForbiddenException('User is not coach of this organization');

  // 2. VÃ©rification de l'existence de la catÃ©gorie avec filtres organisationnels
  const coachFilterConditions = await this.memberUseCases.getCoachFilterConditions(organizationId);
  const category = await this.workoutCategoryRepository.getOne(workout.workoutCategory, coachFilterConditions);

  if (!category) {
    throw new NotFoundException(
      `Workout category with ID ${workout.workoutCategory} not found or access denied`
    );
  }

  // 3. VÃ©rification de l'existence et de l'accÃ¨s aux exercices/complexes
  for (const element of workout.elements) {
    if (element.type === WORKOUT_ELEMENT_TYPES.EXERCISE) {
      const exercise = await this.exerciseRepository.getOne(element.id, coachFilterConditions);
      if (!exercise) {
        throw new NotFoundException(`Exercise with ID ${element.id} not found or access denied`);
      }
    } else {
      const complex = await this.complexRepository.getOne(element.id, coachFilterConditions);
      if (!complex) {
        throw new NotFoundException(`Complex with ID ${element.id} not found or access denied`);
      }
    }
  }

  // 4. Si une session d'entraÃ®nement est demandÃ©e, vÃ©rifier l'existence des athlÃ¨tes
  if (workout.trainingSession) {
    for (const athleteId of workout.trainingSession.athleteIds) {
      const athlete = await this.athleteRepository.getOne(athleteId);
      if (!athlete) {
        throw new NotFoundException(`Athlete with ID ${athleteId} not found`);
      }
    }
  }

  // 4. CrÃ©ation avec logique d'orchestration
  const createdWorkout = await this.workoutRepository.save(workoutToCreate);
  
  // 5. Transformation pour l'exposition
  const workoutDto = WorkoutMapper.toDto(createdWorkout);
  return WorkoutPresenter.presentOne(workoutDto);
}
```

Les Use Cases appliquent des vÃ©rifications mÃ©tier qui nÃ©cessitent l'accÃ¨s aux donnÃ©es. 

Avant de crÃ©er un workout, le Use Case vÃ©rifie systÃ©matiquement que la catÃ©gorie, les exercices et les complexes rÃ©fÃ©rencÃ©s existent ET sont accessibles par le coach via les `coachFilterConditions`. Cette double vÃ©rification empÃªche un coach de crÃ©er un workout utilisant des ressources d'un autre club, garantissant l'isolation des donnÃ©es entre organisations.

Lorsque le workout inclut une session d'entraÃ®nement avec des athlÃ¨tes assignÃ©s, le Use Case vÃ©rifie que chaque `athleteId` correspond Ã  un athlÃ¨te existant en base de donnÃ©es. Cette validation d'intÃ©gritÃ© rÃ©fÃ©rentielle ne peut Ãªtre faite qu'au moment de l'exÃ©cution avec un accÃ¨s effectif aux donnÃ©es, contrairement aux validations de structure que Zod peut effectuer.

Le Use Case orchestre Ã©galement plusieurs rÃ¨gles d'autorisation en combinant diffÃ©rentes vÃ©rifications (coach de l'organisation + accÃ¨s aux ressources spÃ©cifiques) qui nÃ©cessitent des appels Ã  plusieurs repositories. Cette logique d'orchestration dÃ©passe largement le cadre de la validation de schÃ©ma et constitue le cÅ“ur de la logique applicative.

Cette approche centralise la logique mÃ©tier critique tout en la gardant indÃ©pendante de l'infrastructure technique. Les rÃ¨gles d'autorisation et de cohÃ©rence restent les mÃªmes mÃªme si je change de base de donnÃ©es ou d'interface d'exposition.

#### Domain Layer : modÃ¨le mÃ©tier

Les entitÃ©s reprÃ©sentent les concepts mÃ©tier du domaine de l'haltÃ©rophilie avec leurs rÃ¨gles et contraintes. Dans l'implÃ©mentation actuelle, elles utilisent des dÃ©corateurs MikroORM pour dÃ©finir leur mapping vers la base de donnÃ©es :

```typescript
@Entity() // DÃ©corateur qui marque cette classe comme une entitÃ© de base de donnÃ©es
@Check({
  name: 'check_one_element_type',
  expression: `(type = 'exercise' AND exercise_id IS NOT NULL) OR (type = 'complex' AND complex_id IS NOT NULL)`
})
export class WorkoutElement {
  @PrimaryKey({ type: 'uuid', defaultRaw: 'gen_random_uuid()' })
  id!: string;

  @Enum({ items: () => Object.values(WORKOUT_ELEMENT_TYPES) })
  type!: WorkoutElementType; // Enum contraint Ã  'exercise' ou 'complex'
  
  @ManyToOne(() => Exercise, { nullable: true })
  exercise?: Exercise; // Relation optionnelle vers un exercice
  
  @ManyToOne(() => Complex, { nullable: true })
  complex?: Complex; // Relation optionnelle vers un complexe
  
  @Property()
  sets!: number; // Nombre de sÃ©ries
  
  @Property()
  reps!: number; // Nombre de rÃ©pÃ©titions
  
  @Property({ onCreate: () => new Date() })
  createdAt: Date = new Date(); // Timestamp automatique
}
```

Chaque dÃ©corateur MikroORM a un rÃ´le spÃ©cifique dans le mapping objet-relationnel :

Le dÃ©corateur `@Entity()` indique Ã  MikroORM que cette classe TypeScript correspond Ã  une table en base de donnÃ©es. La faÃ§on dont le schÃ©ma sera gÃ©nÃ©rÃ© sera dÃ©taillÃ©e dans la partie configuration MikroORM et Migration.

Les dÃ©corateurs `@Property()` mappent les propriÃ©tÃ©s simples vers des colonnes de base de donnÃ©es. MikroORM infÃ¨re automatiquement le type SQL appropriÃ© (VARCHAR, INTEGER, TIMESTAMP) selon le type TypeScript dÃ©clarÃ©.

Les dÃ©corateurs de relation comme `@ManyToOne()` Ã©tablissent les associations entre entitÃ©s et gÃ©nÃ¨rent automatiquement les clÃ©s Ã©trangÃ¨res correspondantes. L'option `nullable: true` permet d'avoir une relation optionnelle, essentielle pour le pattern polymorphe de `WorkoutElement`.

Le dÃ©corateur `@Check()` traduit une rÃ¨gle mÃ©tier en contrainte PostgreSQL. Dans cet exemple, il garantit qu'un Ã©lÃ©ment de workout rÃ©fÃ©rence soit un exercice, soit un complexe, mais jamais les deux simultanÃ©ment. Cette contrainte de base de donnÃ©es renforce l'intÃ©gritÃ© mÃªme en cas d'accÃ¨s direct aux donnÃ©es.

Les dÃ©corateurs automatiques comme `@Property({ onCreate: () => new Date() })` configurent des comportements de lifecycle. MikroORM mettra automatiquement Ã  jour la date de crÃ©ation lors de l'insertion en base.

Cette approche prÃ©sente cependant une limitation par rapport Ã  l'architecture hexagonale pure. IdÃ©alement, les entitÃ©s du domaine ne devraient contenir aucune dÃ©pendance vers l'infrastructure technique. Une implÃ©mentation strictement hexagonale nÃ©cessiterait la crÃ©ation d'entitÃ©s "pures" sans dÃ©corateurs MikroORM, accompagnÃ©es de mappers sÃ©parÃ©s pour la transformation vers les entitÃ©s de persistence.

Cette sÃ©paration permettrait une indÃ©pendance complÃ¨te vis-Ã -vis de l'ORM choisi. Les entitÃ©s mÃ©tier pourraient Ã©voluer selon les besoins business sans Ãªtre contraintes par les limitations techniques de MikroORM. Un changement vers Prisma, TypeORM ou mÃªme une approche sans ORM ne nÃ©cessiterait que la rÃ©Ã©criture des mappers, laissant intact le cÅ“ur mÃ©tier de l'application.

#### Infrastructure Layer : accÃ¨s aux donnÃ©es

L'Infrastructure Layer contient les **Repositories** qui assurent la persistance des donnÃ©es. MikroORM propose nativement des repositories automatiques pour chaque entitÃ© annotÃ©e `@Entity()`, accessibles directement via l'injection de dÃ©pendance. Ces repositories par dÃ©faut offrent les opÃ©rations CRUD basiques (`findOne`, `find`, `save`, `remove`) sans configuration supplÃ©mentaire.

Pour certains cas spÃ©cifiques, il peut Ãªtre intÃ©ressant d'Ã©tendre ces repositories automatiques. Par exemple, la mÃ©thode `getOneWithDetails` nÃ©cessite un populate profond sur plusieurs niveaux de relations (workout â†’ elements â†’ exercise/complex â†’ categories) avec des conditions de filtrage organisationnel. Cette requÃªte spÃ©cialisÃ©e justifie la crÃ©ation d'un repository personnalisÃ© qui respecte les contrats dÃ©finis par l'Application Layer :

```typescript
@Injectable()
export class MikroWorkoutRepository extends EntityRepository<Workout> implements IWorkoutRepository {
  constructor(public readonly em: EntityManager) {
    super(em, Workout);
  }

  // MÃ©thode spÃ©cialisÃ©e avec populate profond et filtrage organisationnel
  async getOneWithDetails(id: string, coachFilterConditions: CoachFilterConditions): Promise<Workout | null> {
    return await this.em.findOne(
      Workout,
      { id, $or: coachFilterConditions.$or },
      {
        populate: [
          'category',
          'elements',
          'elements.exercise',
          'elements.exercise.exerciseCategory',
          'elements.complex',
          'elements.complex.complexCategory',
          'elements.complex.exercises',
          'elements.complex.exercises.exercise',
          'elements.complex.exercises.exercise.exerciseCategory',
          'createdBy'
        ],
      }
    );
  }
}
```

Cette approche hybride me donne le meilleur des deux mondes. L'hÃ©ritage d'`EntityRepository<Workout>` conserve l'accÃ¨s aux mÃ©thodes MikroORM optimisÃ©es (comme `findOne`, `save`), tandis que l'implÃ©mentation de `IWorkoutRepository` garantit le respect du contrat mÃ©tier dÃ©fini dans l'Application Layer. L'`EntityManager` injectÃ© donne accÃ¨s Ã  toutes les opÃ©rations de persistence avancÃ©es, mais le Repository n'expose aux Use Cases que les mÃ©thodes strictement nÃ©cessaires comme cette requÃªte spÃ©cialisÃ©e `getOneWithDetails` impossible Ã  rÃ©aliser avec les repositories automatiques.

#### Gestion du multi-tenancy

La gestion du multi-tenancy au sein de DropIt prÃ©sente une particularitÃ© importante : chaque coach possÃ¨de son propre catalogue d'exercices personnalisÃ©s qu'il dÃ©veloppe au fil du temps. Cette approche rÃ©pond Ã  un besoin mÃ©tier spÃ©cifique de l'haltÃ©rophilie oÃ¹ les coachs construisent leur mÃ©thode et leurs variantes d'exercices sur le long terme. Si un coach change de club, il doit pouvoir conserver son catalogue personnel sans emporter les athlÃ¨tes de l'ancien club.

Cette logique d'appartenance crÃ©e une double isolation : les donnÃ©es d'organisation (athlÃ¨tes) et les donnÃ©es personnelles de coach (catalogue d'exercices, complexes, programmes). Cette exigence fondamentale impacte chaque requÃªte de base de donnÃ©es.

La solution traditionnelle consisterait Ã  crÃ©er une base de donnÃ©es sÃ©parÃ©e par organisation, mais cette approche pose plusieurs problÃ¨mes concrets. D'abord, la **scalabilitÃ©** devient problÃ©matique : crÃ©er 100 clubs nÃ©cessiterait 100 bases de donnÃ©es identiques avec 100 connexions sÃ©parÃ©es Ã  gÃ©rer. Ensuite, la **maintenance** se complique Ã©normÃ©ment : chaque migration de schÃ©ma doit Ãªtre appliquÃ©e sur toutes les bases, chaque backup doit Ãªtre gÃ©rÃ© individuellement, et la supervision technique devient un cauchemar administratif.

J'ai optÃ© pour une approche de "row-level security" logicielle : plutÃ´t que d'isoler physiquement les donnÃ©es, j'applique des filtres automatiques Ã  chaque requÃªte pour n'afficher que les lignes (rows) auxquelles l'utilisateur a accÃ¨s. Cette sÃ©curitÃ© au niveau des enregistrements s'intÃ¨gre directement dans les requÃªtes via les `CoachFilterConditions`.

PostgreSQL propose nativement des politiques RLS (Row Level Security) qui pourraient automatiser ce filtrage directement au niveau de la base de donnÃ©es. Cependant, cette approche aurait nÃ©cessitÃ© une gestion complexe des contextes utilisateur au niveau SQL et une coordination dÃ©licate avec l'architecture NestJS. L'implÃ©mentation logicielle me donne plus de flexibilitÃ© pour adapter les rÃ¨gles d'accÃ¨s selon l'Ã©volution des besoins mÃ©tier, tout en conservant une logique centralisÃ©e dans les Use Cases.

Chaque entitÃ© possÃ¨de un champ `createdBy` qui rÃ©fÃ©rence l'utilisateur crÃ©ateur. Les conditions de filtrage appliquent automatiquement les rÃ¨gles d'isolation organisationnelle :

```typescript
export type CoachFilterConditions = {
  $or: [
    { createdBy: null }, // EntitÃ©s publiques (exercices de base, seedings)
    { createdBy: { id: { $in: string[] } } }, // EntitÃ©s crÃ©Ã©es par les coachs de l'organisation
  ];
};
```

Cette structure permet deux niveaux d'accÃ¨s. Les entitÃ©s publiques (`createdBy: null`) correspondent aux donnÃ©es de base injectÃ©es par les seeders : exercices officiels d'haltÃ©rophilie, catÃ©gories standard, etc. Ces ressources sont accessibles Ã  tous les clubs. Les entitÃ©s privÃ©es appartiennent Ã  des coachs spÃ©cifiques et ne sont accessibles qu'aux membres de la mÃªme organisation.

La vÃ©rification d'appartenance organisationnelle s'effectue dans l'Application Layer via `MemberUseCases.getCoachFilterConditions(organizationId)`, qui rÃ©cupÃ¨re la liste des IDs de tous les coachs de l'organisation courante. Cette liste alimente ensuite le filtre `{ createdBy: { id: { $in: string[] } } }` appliquÃ© systÃ©matiquement Ã  chaque opÃ©ration de Repository.

Cette approche dÃ©fensive garantit qu'mÃªme si un utilisateur tente d'accÃ©der Ã  un workout qui ne lui appartient pas (que ce soit par erreur de requÃªte, bug dans l'interface, ou tentative malveillante), la requÃªte retournera `null` car les conditions de filtrage l'excluront automatiquement. La sÃ©curitÃ© est ainsi assurÃ©e mÃªme en cas de faille potentielle dans la couche de prÃ©sentation ou d'authentification, crÃ©ant une dÃ©fense en profondeur au niveau de la persistance des donnÃ©es.

#### GÃ©nÃ©ration automatique des requÃªtes

MikroORM s'appuie sur Knex.js, une bibliothÃ¨que JavaScript de construction de requÃªtes SQL (query builder), pour transformer les opÃ©rations TypeScript en requÃªtes PostgreSQL. Cette couche d'abstraction permet Ã  MikroORM de gÃ©nÃ©rer automatiquement les requÃªtes optimisÃ©es sans que je doive les Ã©crire manuellement.

Lorsque j'utilise l'option `populate` pour charger les relations d'une entitÃ©, MikroORM gÃ©nÃ¨re automatiquement toutes les jointures nÃ©cessaires en une seule requÃªte plutÃ´t que d'exÃ©cuter une requÃªte par relation. Cette optimisation Ã©vite le "problÃ¨me N+1" : au lieu de faire 1 requÃªte pour rÃ©cupÃ©rer un workout puis N requÃªtes supplÃ©mentaires pour rÃ©cupÃ©rer chacun de ses Ã©lÃ©ments, MikroORM gÃ©nÃ¨re une seule requÃªte avec toutes les jointures LEFT nÃ©cessaires.

Par exemple, pour rÃ©cupÃ©rer un workout avec tous ses Ã©lÃ©ments et leurs relations, une approche naÃ¯ve nÃ©cessiterait potentiellement des dizaines de requÃªtes sÃ©parÃ©es. Avec le populate, MikroORM gÃ©nÃ¨re une seule requÃªte avec toutes les jointures, rÃ©duisant drastiquement les aller-retours avec la base de donnÃ©es et amÃ©liorant les performances.

Cette couche Infrastructure isole complÃ¨tement la logique mÃ©tier des dÃ©tails techniques de persistence. Je peux changer d'ORM (vers Prisma, TypeORM) ou de base de donnÃ©es (vers MySQL, SQLite) sans impacter les Use Cases, seuls les Repositories devront Ãªtre rÃ©implÃ©mentÃ©s en respectant les mÃªmes interfaces.

#### BÃ©nÃ©fices de l'architecture en couches

Cette sÃ©paration en couches rÃ©sout plusieurs problÃ¨mes que j'ai identifiÃ©s dans des architectures plus simples. La testabilitÃ© s'amÃ©liore considÃ©rablement car chaque couche peut Ãªtre testÃ©e indÃ©pendamment en simulant ses dÃ©pendances : les Use Cases se testent sans connexion Ã  la base de donnÃ©es, les Controllers sans logique mÃ©tier complexe. 

L'Ã©volutivitÃ© devient naturelle puisque la modification d'une couche n'impacte pas les autres. Je peux faire Ã©voluer le modÃ¨le de donnÃ©es sans toucher Ã  l'API REST, ou changer l'interface d'exposition sans modifier la logique mÃ©tier. Cette indÃ©pendance facilite grandement la maintenance et l'ajout de nouvelles fonctionnalitÃ©s.

La rÃ©utilisabilitÃ© constitue Ã©galement un avantage important. Les Use Cases peuvent Ãªtre rÃ©utilisÃ©s par diffÃ©rentes interfaces d'exposition (API REST, GraphQL, interface en ligne de commande) sans duplication de code. Cette architecture modulaire me permet d'envisager sereinement l'Ã©volution technique de DropIt selon les besoins futurs.

### Flux de donnÃ©es

Pour illustrer concrÃ¨tement cette architecture, voici le trajet d'une requÃªte simple de rÃ©cupÃ©ration d'un workout :

```mermaid
sequenceDiagram
    participant Client as ğŸŒ Client Web
    participant Controller as ğŸ›ï¸ WorkoutController
    participant UseCase as ğŸ“‹ WorkoutUseCases
    participant Repo as ğŸ“¦ MikroWorkoutRepository
    participant ORM as ğŸ”„ MikroORM
    participant DB as ğŸ’¾ PostgreSQL
    participant Mapper as ğŸ”„ WorkoutMapper
    participant Presenter as ğŸ“¤ WorkoutPresenter

    Client->>Controller: GET /api/workouts/123
    Controller->>UseCase: getWorkoutWithDetails(id, orgId, userId)
    
    UseCase->>UseCase: VÃ©rification permissions coach
    UseCase->>Repo: getOneWithDetails(id, filterConditions)
    
    Repo->>ORM: em.findOne(Workout, conditions, populate)
    ORM->>DB: SELECT avec LEFT JOIN (auto-gÃ©nÃ©rÃ©e)
    DB-->>ORM: RÃ©sultat SQL brut
    ORM-->>Repo: EntitÃ© Workout hydratÃ©e
    
    Repo-->>UseCase: Workout avec relations
    UseCase->>Mapper: WorkoutMapper.toDto(workout)
    Mapper-->>UseCase: WorkoutDto typÃ©
    
    UseCase->>Presenter: WorkoutPresenter.presentOne(dto)
    Presenter-->>UseCase: Response formatÃ©e
    UseCase-->>Controller: Response
    Controller-->>Client: HTTP 200 + JSON
```

Ce diagramme illustre comment chaque couche a sa responsabilitÃ© spÃ©cifique : le Controller gÃ¨re le protocole HTTP, le UseCase orchestre la logique mÃ©tier et les permissions, le Repository abstrait l'accÃ¨s aux donnÃ©es, et le Mapper/Presenter formatent les donnÃ©es pour le client.

Cette approche technique avec MikroORM reprÃ©sente un choix dÃ©libÃ©rÃ© face aux alternatives que je maÃ®trise. L'Ã©criture manuelle de requÃªtes SQL m'aurait permis un contrÃ´le fin sur les performances et l'optimisation, mais au prix d'une complexitÃ© de maintenance importante. Les requÃªtes avec jointures multiples nÃ©cessitent une gestion minutieuse du mapping vers les objets TypeScript et une logique de regroupement complexe pour reconstruire les hiÃ©rarchies relationnelles.

En optant pour MikroORM, j'ai privilÃ©giÃ© la productivitÃ© de dÃ©veloppement et la sÃ©curitÃ© du typage strict. L'ORM Ã©limine le mapping manuel, dÃ©tecte les erreurs de relations Ã  la compilation, et gÃ©nÃ¨re automatiquement les jointures optimisÃ©es. Cette approche reste flexible : pour des cas particuliers nÃ©cessitant des optimisations spÃ©cifiques, l'EntityManager permet toujours d'Ã©crire des requÃªtes SQL manuelles quand le populate atteint ses limites.

## Pattern Unit of Work et gestion transactionnelle

### Le pattern Unit of Work

Le pattern Unit of Work consiste Ã  maintenir une liste de tous les objets modifiÃ©s pendant une transaction et Ã  coordonner leur Ã©criture en base de donnÃ©es en une seule fois. PlutÃ´t que de sauvegarder chaque modification immÃ©diatement, ce pattern accumule les changements en mÃ©moire puis les applique tous ensemble lors d'un "flush".

MikroORM implÃ©mente nativement ce pattern : lorsque je modifie une entitÃ© chargÃ©e, elle est automatiquement marquÃ©e comme "dirty" sans dÃ©clencher immÃ©diatement une requÃªte SQL. C'est seulement lors de l'appel Ã  `flush()` que toutes les modifications sont synchronisÃ©es avec la base de donnÃ©es dans l'ordre appropriÃ©.

### Transactions et propriÃ©tÃ©s ACID

Les transactions garantissent les propriÃ©tÃ©s ACID (Atomicity, Consistency, Isolation, Durability) essentielles pour l'intÃ©gritÃ© des donnÃ©es :

- **AtomicitÃ©** : Soit toutes les opÃ©rations rÃ©ussissent, soit aucune n'est appliquÃ©e
- **CohÃ©rence** : Les contraintes de base de donnÃ©es sont respectÃ©es Ã  la fin de la transaction
- **Isolation** : Les transactions concurrentes n'interfÃ¨rent pas entre elles
- **DurabilitÃ©** : Une fois validÃ©e, la transaction persiste mÃªme en cas de panne systÃ¨me

Dans le contexte de DropIt, cela signifie qu'un workout ne peut pas Ãªtre crÃ©Ã© avec des Ã©lÃ©ments orphelins, ou qu'un athlÃ¨te ne peut pas Ãªtre supprimÃ© s'il participe encore Ã  des sessions d'entraÃ®nement.

### Fonctionnement automatique avec NestJS

MikroORM s'intÃ¨gre avec le systÃ¨me d'intercepteurs de NestJS pour fournir automatiquement une transaction par requÃªte HTTP. Techniquement, l'intercepteur `RequestContext` de MikroORM encapsule chaque requÃªte HTTP entrante dans un contexte transactionnel : il crÃ©e automatiquement un `EntityManager` avec une transaction ouverte, l'associe au thread de traitement de la requÃªte, puis commit automatiquement si tout se passe bien ou rollback en cas d'erreur :

```typescript
async save(workout: Workout): Promise<Workout> {
  await this.em.persistAndFlush(workout); // Persiste et flush dans la transaction courante
  return workout;
}
```

L'`EntityManager` suit automatiquement les modifications apportÃ©es aux entitÃ©s chargÃ©es et gÃ©nÃ¨re les requÃªtes SQL optimales lors du flush. Cette approche rÃ©duit le nombre d'aller-retours avec la base de donnÃ©es et garantit la cohÃ©rence transactionnelle.


### Gestion des suppressions en cascade

La suppression d'entitÃ©s avec des relations nÃ©cessite une gestion particuliÃ¨re pour respecter l'intÃ©gritÃ© rÃ©fÃ©rentielle. Dans DropIt, un workout possÃ¨de des Ã©lÃ©ments liÃ©s via une clÃ© Ã©trangÃ¨re : supprimer le workout sans gÃ©rer ces Ã©lÃ©ments violerait les contraintes de base de donnÃ©es.

MikroORM propose plusieurs stratÃ©gies pour gÃ©rer ces suppressions. J'ai optÃ© pour une approche explicite qui me donne le contrÃ´le total sur l'ordre des opÃ©rations :

```typescript
async remove(id: string, coachFilterConditions: CoachFilterConditions): Promise<void> {
  const workoutToDelete = await this.em.findOne(
    Workout,
    { id, $or: coachFilterConditions.$or },
    { populate: ['elements'] }
  );
  
  if (!workoutToDelete) {
    return;
  }

  // Suppression explicite des Ã©lÃ©ments pour respecter les contraintes
  const elements = workoutToDelete.elements.getItems();
  for (const element of elements) {
    this.em.remove(element);
  }

  await this.em.removeAndFlush(workoutToDelete);
}
```

Cette gestion manuelle me permet d'Ã©viter les contraintes CASCADE au niveau SQL ou les dÃ©corateurs `onDelete: 'cascade'` de MikroORM. Sans cette approche, PostgreSQL rejetterait la suppression du workout avec une erreur de contrainte de clÃ© Ã©trangÃ¨re, puisque des Ã©lÃ©ments y font encore rÃ©fÃ©rence.

L'alternative serait de dÃ©finir `onDelete: 'cascade'` sur la relation `@OneToMany`, ce qui dÃ©lÃ©guerait la suppression en cascade Ã  MikroORM. Cependant, la suppression manuelle me donne plus de contrÃ´le sur le processus : je peux facilement ajouter des logs pour tracer les suppressions, valider des rÃ¨gles mÃ©tier avant chaque suppression, ou mÃªme implÃ©menter une suppression "soft" en marquant les entitÃ©s comme supprimÃ©es sans les effacer physiquement.

Cette flexibilitÃ© s'avÃ¨re particuliÃ¨rement utile dans un contexte professionnel oÃ¹ les exigences de traÃ§abilitÃ© et d'audit sont importantes pour la gestion des donnÃ©es sportives.

## SÃ©curitÃ© applicative et protection OWASP

L'architecture que j'ai mise en place intÃ¨gre des mesures de sÃ©curitÃ© spÃ©cifiques pour contrer les principales vulnÃ©rabilitÃ©s rÃ©pertoriÃ©es par l'OWASP. L'utilisation de MikroORM avec des requÃªtes paramÃ©trÃ©es, combinÃ©e Ã  la validation stricte des entrÃ©es via les DTOs Zod, Ã©limine efficacement les risques d'injection SQL (OWASP A03). Les schÃ©mas de validation partagÃ©s entre frontend et backend garantissent une validation cohÃ©rente Ã  tous les niveaux de l'application.

Concernant le contrÃ´le d'accÃ¨s (OWASP A01), chaque endpoint bÃ©nÃ©ficie de la protection des guards NestJS qui vÃ©rifient systÃ©matiquement les permissions utilisateur via le package `@dropit/permissions`. L'isolation par organisation garantit que les utilisateurs ne peuvent accÃ©der qu'aux donnÃ©es de leur club respectif, empÃªchant tout accÃ¨s transversal non autorisÃ©.

La validation et la sanitisation des donnÃ©es (OWASP A04) s'effectuent grÃ¢ce aux schÃ©mas Zod stricts dÃ©finis dans `@dropit/schemas`, assurant une validation uniforme entre toutes les couches applicatives. Cette approche centralisÃ©e Ã©vite les disparitÃ©s de validation qui pourraient crÃ©er des failles de sÃ©curitÃ©.

## Configuration et optimisations

### Configuration MikroORM adaptÃ©e aux environnements

La configuration centralisÃ©e dans `mikro-orm.config.ts` s'adapte selon l'environnement d'exÃ©cution :

```typescript
export function createMikroOrmOptions(options?: CreateMikroOrmOptions) {
  const { isTest, ...restOptions } = options ?? {};
  const isTestEnvironment = isTest || config.env === 'test';

  return defineConfig({
    entities: ['./dist/**/*.entity.js'],
    entitiesTs: ['./src/**/*.entity.ts'],
    dbName: config.database.name,
    host: config.database.host,
    port: config.database.port,
    user: config.database.user,
    password: config.database.password,
    metadataProvider: TsMorphMetadataProvider,
    forceUtcTimezone: true,
    extensions: [SeedManager, Migrator],
    debug: config.env === 'development', // Logs SQL uniquement en dÃ©veloppement
    allowGlobalContext: isTestEnvironment,
  });
}
```

Cette configuration rÃ©vÃ¨le plusieurs optimisations importantes selon l'environnement :

**DÃ©couverte automatique des entitÃ©s** : La configuration `entities` et `entitiesTs` permet Ã  MikroORM de dÃ©couvrir automatiquement toutes les classes annotÃ©es `@Entity()` via l'analyse des patterns de fichiers. En dÃ©veloppement, MikroORM utilise `entitiesTs` pour analyser directement les fichiers TypeScript, tandis qu'en production, il se base sur `entities` pointant vers les fichiers JavaScript compilÃ©s.

**Analyse statique performante** : Le `TsMorphMetadataProvider` analyse le code TypeScript Ã  la compilation plutÃ´t qu'au runtime, Ã©liminant le besoin de dÃ©corateurs reflect-metadata coÃ»teux. Cette approche rÃ©duit significativement le temps de dÃ©marrage de l'application et son empreinte mÃ©moire en production.

**CohÃ©rence temporelle** : `forceUtcTimezone: true` garantit que toutes les dates sont stockÃ©es et manipulÃ©es en UTC, Ã©vitant les problÃ¨mes de fuseaux horaires lors des dÃ©ploiements multi-rÃ©gions ou des changements d'heure saisonniers.

### Gestion des migrations en production

La stratÃ©gie de migration adoptÃ©e privilÃ©gie la sÃ©curitÃ© et la traÃ§abilitÃ© en environnement de production :

```typescript
migrations: {
  path: './dist/modules/db/migrations',
  pathTs: './src/modules/db/migrations',
  allOrNothing: true, // Transactions atomiques
  disableForeignKeys: false, // PrÃ©servation de l'intÃ©gritÃ©
},
```

**GÃ©nÃ©ration automatique en dÃ©veloppement** : Le processus `npm run db:migration:create` gÃ©nÃ¨re automatiquement les fichiers de migration en analysant les diffÃ©rences entre les entitÃ©s TypeScript et le schÃ©ma de base de donnÃ©es actuel. Cette automatisation Ã©limine les erreurs humaines dans la crÃ©ation des scripts de migration.

**Application atomique** : Le paramÃ¨tre `allOrNothing: true` encapsule l'application de toutes les migrations en attente dans une transaction unique. Si une migration Ã©choue, toutes les modifications sont annulÃ©es, garantissant que la base de donnÃ©es ne reste jamais dans un Ã©tat incohÃ©rent.

**PrÃ©servation des contraintes** : `disableForeignKeys: false` maintient l'intÃ©gritÃ© rÃ©fÃ©rentielle pendant les migrations. Cette approche plus sÃ»re peut nÃ©cessiter un ordre spÃ©cifique dans certaines migrations complexes, mais elle prÃ©vient toute corruption de donnÃ©es.

**TraÃ§abilitÃ© complÃ¨te** : Chaque migration appliquÃ©e est enregistrÃ©e dans une table systÃ¨me, permettant de connaÃ®tre l'Ã©tat exact du schÃ©ma Ã  tout moment. Cette traÃ§abilitÃ© s'avÃ¨re cruciale lors des dÃ©ploiements en production pour valider l'Ã©tat de la base de donnÃ©es.

### StratÃ©gie diffÃ©renciÃ©e selon l'environnement

La gestion du schÃ©ma de base de donnÃ©es suit une stratÃ©gie adaptÃ©e aux contraintes de chaque environnement.

En dÃ©veloppement, j'ai privilÃ©giÃ© une approche de reconstruction complÃ¨te via les seeders. Cette mÃ©thode permet de tester rapidement les modifications de schÃ©ma en supprimant et recrÃ©ant toutes les tables avec des donnÃ©es cohÃ©rentes. Cette flexibilitÃ© s'avÃ¨re particuliÃ¨rement utile lors des phases d'itÃ©ration rapide sur le modÃ¨le de donnÃ©es.

En production, cette approche n'est Ã©videmment pas envisageable car elle dÃ©truirait toutes les donnÃ©es utilisateur. Le systÃ¨me de migrations devient alors indispensable pour faire Ã©voluer le schÃ©ma tout en prÃ©servant l'intÃ©gritÃ© et la continuitÃ© des donnÃ©es des clubs et de leurs athlÃ¨tes. 

Les migrations gÃ©nÃ©rÃ©es automatiquement par MikroORM peuvent Ãªtre vÃ©rifiÃ©es avant application, conservant ainsi le contrÃ´le sur les modifications appliquÃ©es :

```typescript
import { Migration } from '@mikro-orm/migrations';

export class Migration20240115000000 extends Migration {

  async up(): Promise<void> {
    this.addSql('alter table "workout" add column "difficulty_level" int null;');
    this.addSql('alter table "workout" add constraint "workout_difficulty_level_check" check ("difficulty_level" >= 1 and "difficulty_level" <= 5);');
  }

  async down(): Promise<void> {
    this.addSql('alter table "workout" drop constraint "workout_difficulty_level_check";');
    this.addSql('alter table "workout" drop column "difficulty_level";');
  }

}
```

Ce script gÃ©nÃ©rÃ© automatiquement rÃ©vÃ¨le plusieurs aspects importants : la mÃ©thode `up()` applique les modifications (ajout de colonne et contrainte), tandis que `down()` permet un rollback propre si nÃ©cessaire. Cette transparence me permet de valider chaque modification SQL avant de l'appliquer en production, combinant l'automatisation avec le contrÃ´le manuel.

## Seeders et donnÃ©es de test

Pour faciliter le dÃ©veloppement et les tests, j'ai implÃ©mentÃ© un systÃ¨me de seeders qui peuple la base avec des donnÃ©es cohÃ©rentes. Ces seeders servent un double objectif : fournir un environnement de dÃ©veloppement reproductible et crÃ©er un catalogue commun d'exercices et de techniques d'haltÃ©rophilie accessible Ã  tous les clubs :

```typescript
export async function seedComplexes(em: EntityManager): Promise<Complex[]> {
  const exercisesMap = await seedExercises(em); // DÃ©pendance des exercices

  const complexCategories = [
    { name: 'ArrachÃ©', description: "Exercices focalisÃ©s sur la technique de l'arrachÃ©" },
    { name: 'Ã‰paulÃ©', description: "Exercices focalisÃ©s sur la technique de l'Ã©paulÃ©-jetÃ©" },
    { name: 'Renforcement', description: 'Exercices de musculation spÃ©cifiques' },
  ];

  // CrÃ©ation des catÃ©gories
  const complexCategoriesMap: Record<string, ComplexCategory> = {};
  for (const complexCategory of complexCategories) {
    const categoryToCreate = new ComplexCategory();
    categoryToCreate.name = complexCategory.name;
    categoryToCreate.createdBy = null;
    await em.persistAndFlush(categoryToCreate);
    complexCategoriesMap[complexCategory.name] = categoryToCreate;
  }

  // CrÃ©ation des complexes avec leurs exercices
  const complexesToCreate = [
    {
      category: 'ArrachÃ©',
      description: "Focus sur la technique de l'arrachÃ©",
      exercises: [
        { name: 'ArrachÃ© Debout', reps: 3 },
        { name: 'Tirage Nuque', reps: 5 },
        { name: 'Squat Clavicule', reps: 2 },
      ],
    },
    // Autres complexes...
  ];

  const complexesCreated: Complex[] = [];
  for (const complexData of complexesToCreate) {
    const complex = new Complex();
    complex.description = complexData.description;
    complex.complexCategory = complexCategoriesMap[complexData.category];
    
    await em.persistAndFlush(complex);

    // CrÃ©ation des relations exercice-complexe avec ordre
    for (let i = 0; i < complexData.exercises.length; i++) {
      const exerciseData = complexData.exercises[i];
      const exerciseComplex = new ExerciseComplex();
      exerciseComplex.complex = complex;
      exerciseComplex.exercise = exercisesMap[exerciseData.name];
      exerciseComplex.order = i;
      exerciseComplex.reps = exerciseData.reps;
      
      await em.persistAndFlush(exerciseComplex);
    }
    
    complexesCreated.push(complex);
  }

  return complexesCreated;
}
```

Ce systÃ¨me de seeders respecte les contraintes d'intÃ©gritÃ© rÃ©fÃ©rentielle et garantit un environnement de dÃ©veloppement reproductible. La structure modulaire permet de rÃ©utiliser les donnÃ©es entre diffÃ©rents seeders tout en maintenant la cohÃ©rence des relations.

L'aspect particuliÃ¨rement intÃ©ressant de ces seeders est leur rÃ´le dans la crÃ©ation de ressources partagÃ©es via `createdBy = null`. Ces entitÃ©s publiques constituent un socle commun d'exercices officiels d'haltÃ©rophilie (ArrachÃ©, Ã‰paulÃ©-JetÃ©, Squat) et de complexes techniques que tous les clubs peuvent utiliser. Cette approche Ã©vite la duplication des donnÃ©es de base tout en permettant Ã  chaque coach de crÃ©er ses propres variantes personnalisÃ©es qui lui appartiennent exclusivement.

## Conclusion

Cette implÃ©mentation de la couche d'accÃ¨s aux donnÃ©es avec Nest.js et MikroORM m'a permis de rÃ©soudre les dÃ©fis spÃ©cifiques de DropIt tout en posant les bases d'une architecture Ã©volutive. 

Le choix de l'approche Code First s'est avÃ©rÃ© particuliÃ¨rement adaptÃ© au contexte du monorepo, permettant une cohÃ©rence complÃ¨te avec les packages partagÃ©s et une productivitÃ© optimale en dÃ©veloppement.

Cette architecture en couches, inspirÃ©e des principes hexagonaux, me donne aujourd'hui la flexibilitÃ© nÃ©cessaire pour faire Ã©voluer DropIt selon les besoins futurs des clubs d'haltÃ©rophilie. Que ce soit pour intÃ©grer de nouveaux types de donnÃ©es sportives, Ã©tendre les fonctionnalitÃ©s de planification d'entraÃ®nement, ou migrer vers d'autres technologies, les fondations posÃ©es rÃ©sisteront aux Ã©volutions Ã  venir.

La section suivante sur les [couches de prÃ©sentation](/conception/presentations) explore maintenant comment ces donnÃ©es sont consommÃ©es et prÃ©sentÃ©es aux utilisateurs via les clients web et mobile, complÃ©tant ainsi l'architecture distribuÃ©e de l'application.

