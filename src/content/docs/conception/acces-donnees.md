---
title: Acc√®s aux donn√©es
description: Impl√©mentation de la couche d'acc√®s aux donn√©es avec MikroORM
---

## Approches d'impl√©mentation de la couche de donn√©es

Apr√®s avoir √©tabli le mod√®le conceptuel avec la m√©thode Merise, plusieurs approches s'offraient √† moi pour impl√©menter la couche d'acc√®s aux donn√©es dans DropIt. Chacune pr√©sente des avantages selon le contexte de d√©veloppement et les contraintes techniques du projet.

### Database First

Cette approche aurait consist√© √† cr√©er directement les tables PostgreSQL via des scripts SQL, puis g√©n√©rer les entit√©s TypeScript √† partir du sch√©ma existant. Pour illustrer cette m√©thode, voici comment j'aurais pu cr√©er la table `workout` :

```sql
CREATE TABLE workout (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title VARCHAR(255) NOT NULL,
    description TEXT NOT NULL,
    category_id UUID NOT NULL,
    created_by UUID,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    FOREIGN KEY (category_id) REFERENCES workout_category(id),
    FOREIGN KEY (created_by) REFERENCES users(id)
);

-- Table de jointure polymorphe pour les √©l√©ments de workout
CREATE TABLE workout_element (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    workout_id UUID NOT NULL,
    type VARCHAR(20) NOT NULL CHECK (type IN ('exercise', 'complex')),
    exercise_id UUID,
    complex_id UUID,
    order_position INTEGER NOT NULL,
    sets INTEGER DEFAULT 1,
    reps INTEGER DEFAULT 1,
    rest INTEGER,
    start_weight_percent DECIMAL(5,2),
    FOREIGN KEY (workout_id) REFERENCES workout(id) ON DELETE CASCADE,
    FOREIGN KEY (exercise_id) REFERENCES exercise(id),
    FOREIGN KEY (complex_id) REFERENCES complex(id),
    CONSTRAINT check_one_element_type CHECK (
        (type = 'exercise' AND exercise_id IS NOT NULL AND complex_id IS NULL) OR
        (type = 'complex' AND complex_id IS NOT NULL AND exercise_id IS NULL)
    )
);
```

Cette m√©thode traditionnelle offre un contr√¥le total sur la structure de la base de donn√©es et garantit des performances optimales gr√¢ce √† la ma√Ætrise fine des index et contraintes. Cependant, elle pr√©sente plusieurs inconv√©nients.

La synchronisation entre le sch√©ma de base de donn√©es et le code applicatif devient rapidement probl√©matique. Chaque modification de table n√©cessite une mise √† jour manuelle de l'interface TypeScript correspondante. Si j'ajoute une colonne `difficulty_level` √† la table `workout`, je dois manuellement cr√©er la propri√©t√© dans l'interface `WorkoutEntity`, avec le risque d'oublier cette √©tape ou de mal typer la propri√©t√©.

La maintenance des migrations peut repr√©senter √©galement un d√©fi consid√©rable. Les √©volutions de sch√©ma doivent √™tre g√©r√©es via des scripts SQL √©crits √† la main, avec toute la complexit√© que cela implique. Par exemple, l'ajout d'une contrainte NOT NULL sur une colonne existante n√©cessite de v√©rifier que toutes les donn√©es respectent cette contrainte, nettoyer les valeurs nulles existantes, puis appliquer la contrainte - le tout en pr√©servant la coh√©rence des donn√©es pendant la migration.

Sans outil d'orchestration appropri√©, les diff√©rents environnements peuvent facilement diverger. Un d√©veloppeur qui applique manuellement un script SQL sur sa base locale mais oublie de le commiter dans le syst√®me de versioning cr√©e une divergence silencieuse qui ne se r√©v√®le qu'au moment du d√©ploiement en production.

### Schema First

Une approche interm√©diaire utilise un fichier de d√©finition de sch√©ma central pour g√©n√©rer √† la fois la base de donn√©es et les entit√©s TypeScript. Cette m√©thode d√©finit la structure des donn√©es dans un format d√©claratif neutre, puis g√©n√®re automatiquement les scripts SQL et les classes d'entit√©s correspondantes.

Le principe consiste √† maintenir une source de v√©rit√© unique qui d√©crit les tables, relations et contraintes, √©vitant ainsi les d√©synchronisations entre la base de donn√©es et le code applicatif. Cette solution r√©sout effectivement les probl√®mes de coh√©rence de l'approche Database First.

Cependant, cette approche ne s'harmonise pas id√©alement avec l'architecture monorepo que j'ai mise en place. Les packages partag√©s comme @dropit/schemas d√©finissent d√©j√† des sch√©mas Zod r√©utilis√©s √† travers toutes les applications. Ajouter un fichier de sch√©ma suppl√©mentaire cr√©erait une redondance : j'aurais les sch√©mas Zod pour la validation c√¥t√© client et serveur, plus un sch√©ma s√©par√© pour la g√©n√©ration de base de donn√©es. Cette duplication augmente le risque d'incoh√©rences et complique la maintenance.

De plus, l'int√©gration native avec l'√©cosyst√®me TypeScript se trouve compromise. Les types g√©n√©r√©s depuis un sch√©ma externe ne b√©n√©ficient pas de l'auto-compl√©tion et de la v√©rification de types aussi efficacement que les entit√©s d√©finies directement en TypeScript, et le partage de ces types avec les packages communs du monorepo devient plus complexe.

### Code First

J'ai donc finalement retenu l'approche Code First qui d√©finit les entit√©s directement en TypeScript avec les d√©corateurs MikroORM. Cette m√©thode r√©sout les inconv√©nients des deux approches pr√©c√©dentes en s'int√©grant nativement dans l'√©cosyst√®me du monorepo. Les entit√©s MikroORM peuvent directement r√©utiliser et r√©f√©rencer les types d√©finis dans les packages partag√©s (@dropit/schemas, @dropit/contract), cr√©ant une coh√©rence technique compl√®te.

La g√©n√©ration automatique des migrations √† partir des modifications d'entit√©s √©limine les risques de d√©synchronisation tout en pr√©servant un contr√¥le pr√©cis sur la structure de donn√©es. Cette approche tire √©galement parti de l'auto-compl√©tion et de la v√©rification de types native de TypeScript, facilitant le d√©veloppement et r√©duisant les erreurs de compilation.

## D√©finition des entit√©s MikroORM

Apr√®s avoir justifi√© le choix de l'approche Code First, il convient maintenant d'examiner concr√®tement comment les entit√©s MikroORM traduisent le mod√®le conceptuel en impl√©mentation TypeScript.

Les entit√©s constituent la traduction directe du mod√®le logique de donn√©es en classes TypeScript annot√©es. Chaque entit√© encapsule √† la fois la structure des donn√©es et les relations m√©tier.

### Structure type d'une entit√©

```typescript
@Entity()
export class Workout {
  @PrimaryKey({ type: 'uuid', defaultRaw: 'gen_random_uuid()' })
  id!: string;

  @Property()
  title!: string;

  @Property()
  description!: string;

  @ManyToOne(() => WorkoutCategory)
  category!: WorkoutCategory;

  @ManyToOne(() => User, { nullable: true })
  createdBy!: User | null;

  @OneToMany(() => WorkoutElement, (element) => element.workout)
  elements = new Collection<WorkoutElement>(this);

  @Property({ onCreate: () => new Date() })
  createdAt: Date = new Date();

  @Property({ onUpdate: () => new Date() })
  updatedAt: Date = new Date();
}
```

Cette entit√© `Workout` illustre plusieurs patterns que j'ai adopt√©s syst√©matiquement :

**Identifiants UUID** : L'utilisation de `gen_random_uuid()` √©vite les conflits lors des synchronisations entre environnements et facilite les op√©rations de fusion de donn√©es.

**Relations typ√©es** : Les d√©corateurs `@ManyToOne` et `@OneToMany` √©tablissent les relations avec typage strict, permettant √† TypeScript de d√©tecter les erreurs de navigation d'objets √† la compilation.

**Collections MikroORM** : Les relations one-to-many utilisent le type `Collection<T>` qui encapsule la logique de chargement paresseux et de gestion des relations bidirectionnelles.

**Timestamps automatiques** : Les propri√©t√©s `createdAt` et `updatedAt` s'actualisent automatiquement via les callbacks `onCreate` et `onUpdate`.

### Gestion des relations polymorphes

L'entit√© `WorkoutElement` illustre la r√©solution du pattern polymorphe identifi√© dans le mod√®le logique :

```typescript
@Entity()
@Check({
  name: 'check_one_element_type',
  expression: `
    (type = 'exercise' AND exercise_id IS NOT NULL AND complex_id IS NULL) OR
    (type = 'complex' AND complex_id IS NOT NULL AND exercise_id IS NULL)
  `,
})
export class WorkoutElement {
  @Enum({ items: () => Object.values(WORKOUT_ELEMENT_TYPES) })
  type!: WorkoutElementType;

  @ManyToOne(() => Exercise, { nullable: true })
  exercise?: Exercise;

  @ManyToOne(() => Complex, { nullable: true })
  complex?: Complex;

  @Property()
  sets!: number;

  @Property()
  reps!: number;
  
  // Autres propri√©t√©s communes...
}
```

Le d√©corateur `@Check` traduit la contrainte logique en contrainte PostgreSQL, garantissant l'int√©grit√© des donn√©es m√™me en cas d'acc√®s direct √† la base. Cette approche combine la flexibilit√© du polymorphisme avec la rigueur des contraintes relationnelles.

Pour optimiser les performances futures des requ√™tes sur cette table polymorphe, un index composite sur `(type, exercise_id, complex_id)` pourrait s'av√©rer n√©cessaire selon l'√©volution des volumes. Actuellement, PostgreSQL utilise les index automatiques des cl√©s √©trang√®res, mais si les requ√™tes de filtrage par type deviennent fr√©quentes, cet index sp√©cialis√© acc√©l√©rerait significativement les recherches d'√©l√©ments par discriminant.

## Architecture en couches et pattern Repository

Les entit√©s MikroORM d√©finissent la structure des donn√©es, mais leur utilisation dans l'application n√©cessite une architecture bien organis√©e pour s√©parer les responsabilit√©s et faciliter la maintenance. L'acc√®s aux donn√©es dans DropIt respecte une s√©paration stricte des responsabilit√©s via le pattern Repository et l'architecture hexagonale adopt√©e dans l'API NestJS.

### S√©paration des responsabilit√©s

L'architecture que j'ai mise en place respecte une s√©paration stricte des responsabilit√©s √† travers plusieurs couches distinctes. Chaque composant a un r√¥le pr√©cis que je vais d√©tailler avec des exemples concrets de l'impl√©mentation DropIt.

#### Interface Layer : exposition HTTP

**Controllers** g√®rent uniquement le protocole HTTP et orchestrent les v√©rifications de s√©curit√© avant de d√©l√©guer la logique m√©tier. Ils remplissent plusieurs r√¥les cruciaux :

```typescript
@UseGuards(PermissionsGuard) // 1. Garde globale sur toutes les m√©thodes
@Controller()
export class WorkoutController {
  
  @TsRestHandler(c.getWorkout) // 2. Respect du contrat ts-rest
  @RequirePermissions('read')  // 3. Permission sp√©cifique requise
  getWorkout(
    @CurrentOrganization() organizationId: string, // 4. Extraction contexte organisation
    @CurrentUser() user: AuthenticatedUser         // 5. Extraction utilisateur authentifi√©
  ): ReturnType<typeof tsRestHandler<typeof c.getWorkout>> {
    return tsRestHandler(c.getWorkout, async ({ params }) => {
      // 6. D√©l√©gation imm√©diate vers la logique m√©tier
      return await this.workoutUseCases.getWorkoutWithDetails(params.id, organizationId, user.id);
    });
  }

  @TsRestHandler(c.createWorkout)
  @RequirePermissions('create') // Permission diff√©rente pour la cr√©ation
  createWorkout(
    @CurrentOrganization() organizationId: string,
    @CurrentUser() user: AuthenticatedUser
  ): ReturnType<typeof tsRestHandler<typeof c.createWorkout>> {
    return tsRestHandler(c.createWorkout, async ({ body }) => {
      return await this.workoutUseCases.createWorkout(body, organizationId, user.id);
    });
  }
}
```

Le controller orchestre plusieurs m√©canismes de s√©curit√© en cascade :

**Niveau 1 - Authentification** : Le `PermissionsGuard` v√©rifie que l'utilisateur poss√®de un token valide et extrait ses informations via `@CurrentUser()`.

**Niveau 2 - Isolation organisationnelle** : Le d√©corateur `@CurrentOrganization()` garantit que l'utilisateur ne peut acc√©der qu'aux ressources de son organisation, emp√™chant tout acc√®s transversal entre clubs.

**Niveau 3 - Permissions granulaires** : `@RequirePermissions('read')` v√©rifie que l'utilisateur dispose du droit sp√©cifique requis pour cette action. Un membre simple peut avoir le droit 'read' mais pas 'create' ou 'delete'.

**Niveau 4 - Contrat d'API** : `@TsRestHandler(c.getWorkout)` assure que les param√®tres d'entr√©e et les r√©ponses correspondent exactement au contrat d√©fini dans `@dropit/contract`, garantissant la type safety entre l'API et les clients.

Cette approche multicouche me permet d'appliquer le principe de d√©fense en profondeur : m√™me si une v√©rification √©choue, les autres barri√®res prot√®gent l'acc√®s aux donn√©es. Le controller reste simple et focalis√© sur son r√¥le d'orchestration HTTP, sans jamais contenir de logique m√©tier.

**Mappers** transforment les entit√©s de base de donn√©es en objets de transfert (DTO) pour l'API. Ils remplissent deux r√¥les essentiels :

```typescript
export const WorkoutMapper = {
  toDto(workout: Workout): WorkoutDto {
    return {
      id: workout.id,
      title: workout.title,
      workoutCategory: workout.category.name, // Simplification : juste le nom au lieu de l'objet complet
      description: workout.description,
      elements: workout.elements.getItems().map(/* transformation des √©l√©ments */),
    };
  }
}
```

D'une part, ils **respectent le contrat d'API ts-rest** d√©fini dans `@dropit/contract`. Le type de retour `WorkoutDto` correspond exactement au sch√©ma attendu par les clients, garantissant la coh√©rence entre l'API et les applications web/mobile qui l'utilisent.

D'autre part, ils **prot√®gent le sch√©ma de base de donn√©es** en ne exposant pas directement les structures internes. Par exemple, l'entit√© `Workout` contient une relation compl√®te vers `WorkoutCategory` avec tous ses champs (id, description, createdBy, timestamps), mais le mapper ne expose que le nom de la cat√©gorie. Cette approche √©vite de r√©v√©ler des d√©tails d'impl√©mentation comme les cl√©s √©trang√®res, les champs techniques ou les relations qui ne concernent pas le client.

**Presenters** standardisent le formatage des r√©ponses et g√®rent la logique de pr√©sentation des donn√©es. Ils remplissent plusieurs responsabilit√©s importantes :

```typescript
export const WorkoutPresenter = {
  // Succ√®s avec donn√©es
  presentOne(workout: WorkoutDto) {
    return { status: 200 as const, body: workout };
  },
  
  // Succ√®s avec liste
  presentList(workouts: WorkoutDto[]) {
    return { 
      status: 200 as const, 
      body: workouts 
    };
  },

  // Succ√®s de cr√©ation (code diff√©rent)
  presentCreationSuccess(message: string) {
    return { 
      status: 201 as const, 
      body: { message } 
    };
  },

  // Gestion centralis√©e des erreurs
  presentError(error: Error) {
    if (error instanceof BadRequestException) {
      return { status: 400 as const, body: { message: error.message } };
    }
    if (error instanceof ForbiddenException) {
      return { status: 403 as const, body: { message: error.message } };
    }
    if (error instanceof NotFoundException) {
      return { status: 404 as const, body: { message: error.message } };
    }
    
    // Masquage des erreurs internes en production
    console.error('Workout error:', error);
    return {
      status: 500 as const,
      body: { message: 'An error occurred while processing the request' }
    };
  }
}
```

Le Presenter joue un r√¥le crucial dans plusieurs aspects :

**Normalisation des codes de statut** : Il garantit que chaque type d'op√©ration retourne le code HTTP appropri√© (200 pour lecture, 201 pour cr√©ation, 404 pour non trouv√©). Cette coh√©rence facilite la gestion c√¥t√© client.

**S√©curisation des messages d'erreur** : Le Presenter filtre les erreurs techniques internes pour ne pas exposer de d√©tails d'impl√©mentation au client. Une erreur de base de donn√©es devient un message g√©n√©rique, prot√©geant la s√©curit√© de l'application.

**Centralisation du formatage** : Tous les Use Cases utilisent le m√™me Presenter, garantissant un format de r√©ponse coh√©rent pour l'ensemble de l'API. Si je d√©cide de changer la structure des r√©ponses (ajouter des m√©tadonn√©es, modifier l'enveloppe JSON), un seul point de modification suffit.

**√âvolutivit√© du format de sortie** : Le Presenter pourrait facilement √™tre adapt√© pour produire d'autres formats que JSON : XML, CSV, ou m√™me des templates HTML pour une interface web. Cette flexibilit√© ne n√©cessiterait aucune modification des Use Cases.


#### Application Layer : orchestration m√©tier

**Use Cases** concentrent la logique applicative et les r√®gles m√©tier sp√©cifiques au domaine de l'halt√©rophilie. Ils orchestrent les diff√©rents repositories tout en appliquant des v√©rifications m√©tier critiques pour la s√©curit√© des utilisateurs :

```typescript
async createWorkout(workout: CreateWorkout, organizationId: string, userId: string) {
  // 1. V√©rifications d'autorisation m√©tier
  const isCoach = await this.memberUseCases.isUserCoachInOrganization(userId, organizationId);
  if (!isCoach) throw new ForbiddenException('User is not coach of this organization');

  // 2. V√©rification de l'existence de la cat√©gorie avec filtres organisationnels
  const coachFilterConditions = await this.memberUseCases.getCoachFilterConditions(organizationId);
  const category = await this.workoutCategoryRepository.getOne(workout.workoutCategory, coachFilterConditions);

  if (!category) {
    throw new NotFoundException(
      `Workout category with ID ${workout.workoutCategory} not found or access denied`
    );
  }

  // 3. V√©rification de l'existence et de l'acc√®s aux exercices/complexes
  for (const element of workout.elements) {
    if (element.type === WORKOUT_ELEMENT_TYPES.EXERCISE) {
      const exercise = await this.exerciseRepository.getOne(element.id, coachFilterConditions);
      if (!exercise) {
        throw new NotFoundException(`Exercise with ID ${element.id} not found or access denied`);
      }
    } else {
      const complex = await this.complexRepository.getOne(element.id, coachFilterConditions);
      if (!complex) {
        throw new NotFoundException(`Complex with ID ${element.id} not found or access denied`);
      }
    }
  }

  // 4. Si une session d'entra√Ænement est demand√©e, v√©rifier l'existence des athl√®tes
  if (workout.trainingSession) {
    for (const athleteId of workout.trainingSession.athleteIds) {
      const athlete = await this.athleteRepository.getOne(athleteId);
      if (!athlete) {
        throw new NotFoundException(`Athlete with ID ${athleteId} not found`);
      }
    }
  }

  // 4. Cr√©ation avec logique d'orchestration
  const createdWorkout = await this.workoutRepository.save(workoutToCreate);
  
  // 5. Transformation pour l'exposition
  const workoutDto = WorkoutMapper.toDto(createdWorkout);
  return WorkoutPresenter.presentOne(workoutDto);
}
```

Les Use Cases appliquent des v√©rifications m√©tier qui n√©cessitent l'acc√®s aux donn√©es. 

Avant de cr√©er un workout, le Use Case v√©rifie syst√©matiquement que la cat√©gorie, les exercices et les complexes r√©f√©renc√©s existent ET sont accessibles par le coach via les `coachFilterConditions`. Cette double v√©rification emp√™che un coach de cr√©er un workout utilisant des ressources d'un autre club, garantissant l'isolation des donn√©es entre organisations.

Lorsque le workout inclut une session d'entra√Ænement avec des athl√®tes assign√©s, le Use Case v√©rifie que chaque `athleteId` correspond √† un athl√®te existant en base de donn√©es. Cette validation d'int√©grit√© r√©f√©rentielle ne peut √™tre faite qu'au moment de l'ex√©cution avec un acc√®s effectif aux donn√©es, contrairement aux validations de structure que Zod peut effectuer.

Le Use Case orchestre √©galement plusieurs r√®gles d'autorisation en combinant diff√©rentes v√©rifications (coach de l'organisation + acc√®s aux ressources sp√©cifiques) qui n√©cessitent des appels √† plusieurs repositories. Cette logique d'orchestration d√©passe largement le cadre de la validation de sch√©ma et constitue le c≈ìur de la logique applicative.

Cette approche centralise la logique m√©tier critique tout en la gardant ind√©pendante de l'infrastructure technique. Les r√®gles d'autorisation et de coh√©rence restent les m√™mes m√™me si je change de base de donn√©es ou d'interface d'exposition.

#### Domain Layer : mod√®le m√©tier

Les entit√©s repr√©sentent les concepts m√©tier du domaine de l'halt√©rophilie avec leurs r√®gles et contraintes. Dans l'impl√©mentation actuelle, elles utilisent des d√©corateurs MikroORM pour d√©finir leur mapping vers la base de donn√©es :

```typescript
@Entity() // D√©corateur qui marque cette classe comme une entit√© de base de donn√©es
@Check({
  name: 'check_one_element_type',
  expression: `(type = 'exercise' AND exercise_id IS NOT NULL) OR (type = 'complex' AND complex_id IS NOT NULL)`
})
export class WorkoutElement {
  @PrimaryKey({ type: 'uuid', defaultRaw: 'gen_random_uuid()' })
  id!: string;

  @Enum({ items: () => Object.values(WORKOUT_ELEMENT_TYPES) })
  type!: WorkoutElementType; // Enum contraint √† 'exercise' ou 'complex'
  
  @ManyToOne(() => Exercise, { nullable: true })
  exercise?: Exercise; // Relation optionnelle vers un exercice
  
  @ManyToOne(() => Complex, { nullable: true })
  complex?: Complex; // Relation optionnelle vers un complexe
  
  @Property()
  sets!: number; // Nombre de s√©ries
  
  @Property()
  reps!: number; // Nombre de r√©p√©titions
  
  @Property({ onCreate: () => new Date() })
  createdAt: Date = new Date(); // Timestamp automatique
}
```

Chaque d√©corateur MikroORM a un r√¥le sp√©cifique dans le mapping objet-relationnel :

Le d√©corateur `@Entity()` indique √† MikroORM que cette classe TypeScript correspond √† une table en base de donn√©es. La fa√ßon dont le sch√©ma sera g√©n√©r√© sera d√©taill√©e dans la partie configuration MikroORM et Migration.

Les d√©corateurs `@Property()` mappent les propri√©t√©s simples vers des colonnes de base de donn√©es. MikroORM inf√®re automatiquement le type SQL appropri√© (VARCHAR, INTEGER, TIMESTAMP) selon le type TypeScript d√©clar√©.

Les d√©corateurs de relation comme `@ManyToOne()` √©tablissent les associations entre entit√©s et g√©n√®rent automatiquement les cl√©s √©trang√®res correspondantes. L'option `nullable: true` permet d'avoir une relation optionnelle, essentielle pour le pattern polymorphe de `WorkoutElement`.

Le d√©corateur `@Check()` traduit une r√®gle m√©tier en contrainte PostgreSQL. Dans cet exemple, il garantit qu'un √©l√©ment de workout r√©f√©rence soit un exercice, soit un complexe, mais jamais les deux simultan√©ment. Cette contrainte de base de donn√©es renforce l'int√©grit√© m√™me en cas d'acc√®s direct aux donn√©es.

Les d√©corateurs automatiques comme `@Property({ onCreate: () => new Date() })` configurent des comportements de lifecycle. MikroORM mettra automatiquement √† jour la date de cr√©ation lors de l'insertion en base.

Cette approche pr√©sente cependant une limitation par rapport √† l'architecture hexagonale pure. Id√©alement, les entit√©s du domaine ne devraient contenir aucune d√©pendance vers l'infrastructure technique. Une impl√©mentation strictement hexagonale n√©cessiterait la cr√©ation d'entit√©s "pures" sans d√©corateurs MikroORM, accompagn√©es de mappers s√©par√©s pour la transformation vers les entit√©s de persistence.

Cette s√©paration permettrait une ind√©pendance compl√®te vis-√†-vis de l'ORM choisi. Les entit√©s m√©tier pourraient √©voluer selon les besoins business sans √™tre contraintes par les limitations techniques de MikroORM. Un changement vers Prisma, TypeORM ou m√™me une approche sans ORM ne n√©cessiterait que la r√©√©criture des mappers, laissant intact le c≈ìur m√©tier de l'application.

#### Infrastructure Layer : acc√®s aux donn√©es

L'Infrastructure Layer contient les **Repositories** qui assurent la persistance des donn√©es. MikroORM propose nativement des repositories automatiques pour chaque entit√© annot√©e `@Entity()`, accessibles directement via l'injection de d√©pendance. Ces repositories par d√©faut offrent les op√©rations CRUD basiques (`findOne`, `find`, `save`, `remove`) sans configuration suppl√©mentaire.

Pour certains cas sp√©cifiques, il peut √™tre int√©ressant d'√©tendre ces repositories automatiques. Par exemple, la m√©thode `getOneWithDetails` n√©cessite un populate profond sur plusieurs niveaux de relations (workout ‚Üí elements ‚Üí exercise/complex ‚Üí categories) avec des conditions de filtrage organisationnel. Cette requ√™te sp√©cialis√©e justifie la cr√©ation d'un repository personnalis√© qui respecte les contrats d√©finis par l'Application Layer :

```typescript
@Injectable()
export class MikroWorkoutRepository extends EntityRepository<Workout> implements IWorkoutRepository {
  constructor(public readonly em: EntityManager) {
    super(em, Workout);
  }

  // M√©thode sp√©cialis√©e avec populate profond et filtrage organisationnel
  async getOneWithDetails(id: string, coachFilterConditions: CoachFilterConditions): Promise<Workout | null> {
    return await this.em.findOne(
      Workout,
      { id, $or: coachFilterConditions.$or },
      {
        populate: [
          'category',
          'elements',
          'elements.exercise',
          'elements.exercise.exerciseCategory',
          'elements.complex',
          'elements.complex.complexCategory',
          'elements.complex.exercises',
          'elements.complex.exercises.exercise',
          'elements.complex.exercises.exercise.exerciseCategory',
          'createdBy'
        ],
      }
    );
  }
}
```

Cette approche hybride me donne le meilleur des deux mondes. L'h√©ritage d'`EntityRepository<Workout>` conserve l'acc√®s aux m√©thodes MikroORM optimis√©es (comme `findOne`, `save`), tandis que l'impl√©mentation de `IWorkoutRepository` garantit le respect du contrat m√©tier d√©fini dans l'Application Layer. L'`EntityManager` inject√© donne acc√®s √† toutes les op√©rations de persistence avanc√©es, mais le Repository n'expose aux Use Cases que les m√©thodes strictement n√©cessaires comme cette requ√™te sp√©cialis√©e `getOneWithDetails` impossible √† r√©aliser avec les repositories automatiques.

#### Gestion du multi-tenancy

La gestion du multi-tenancy au sein de DropIt pr√©sente une particularit√© importante : chaque coach poss√®de son propre catalogue d'exercices personnalis√©s qu'il d√©veloppe au fil du temps. Cette approche r√©pond √† un besoin m√©tier sp√©cifique de l'halt√©rophilie o√π les coachs construisent leur m√©thode et leurs variantes d'exercices sur le long terme. Si un coach change de club, il doit pouvoir conserver son catalogue personnel sans emporter les athl√®tes de l'ancien club.

Cette logique d'appartenance cr√©e une double isolation : les donn√©es d'organisation (athl√®tes) et les donn√©es personnelles de coach (catalogue d'exercices, complexes, programmes). Cette exigence fondamentale impacte chaque requ√™te de base de donn√©es.

La solution traditionnelle consisterait √† cr√©er une base de donn√©es s√©par√©e par organisation, mais cette approche pose plusieurs probl√®mes concrets. D'abord, la **scalabilit√©** devient probl√©matique : cr√©er 100 clubs n√©cessiterait 100 bases de donn√©es identiques avec 100 connexions s√©par√©es √† g√©rer. Ensuite, la **maintenance** se complique √©norm√©ment : chaque migration de sch√©ma doit √™tre appliqu√©e sur toutes les bases, chaque backup doit √™tre g√©r√© individuellement, et la supervision technique devient un cauchemar administratif.

J'ai opt√© pour une approche de "row-level security" logicielle : plut√¥t que d'isoler physiquement les donn√©es, j'applique des filtres automatiques √† chaque requ√™te pour n'afficher que les lignes (rows) auxquelles l'utilisateur a acc√®s. Cette s√©curit√© au niveau des enregistrements s'int√®gre directement dans les requ√™tes via les `CoachFilterConditions`.

PostgreSQL propose nativement des politiques RLS (Row Level Security) qui pourraient automatiser ce filtrage directement au niveau de la base de donn√©es. Cependant, cette approche aurait n√©cessit√© une gestion complexe des contextes utilisateur au niveau SQL et une coordination d√©licate avec l'architecture NestJS. L'impl√©mentation logicielle me donne plus de flexibilit√© pour adapter les r√®gles d'acc√®s selon l'√©volution des besoins m√©tier, tout en conservant une logique centralis√©e dans les Use Cases.

Chaque entit√© poss√®de un champ `createdBy` qui r√©f√©rence l'utilisateur cr√©ateur. Les conditions de filtrage appliquent automatiquement les r√®gles d'isolation organisationnelle :

```typescript
export type CoachFilterConditions = {
  $or: [
    { createdBy: null }, // Entit√©s publiques (exercices de base, seedings)
    { createdBy: { id: { $in: string[] } } }, // Entit√©s cr√©√©es par les coachs de l'organisation
  ];
};
```

Cette structure permet deux niveaux d'acc√®s. Les entit√©s publiques (`createdBy: null`) correspondent aux donn√©es de base inject√©es par les seeders : exercices officiels d'halt√©rophilie, cat√©gories standard, etc. Ces ressources sont accessibles √† tous les clubs. Les entit√©s priv√©es appartiennent √† des coachs sp√©cifiques et ne sont accessibles qu'aux membres de la m√™me organisation.

La v√©rification d'appartenance organisationnelle s'effectue dans l'Application Layer via `MemberUseCases.getCoachFilterConditions(organizationId)`, qui r√©cup√®re la liste des IDs de tous les coachs de l'organisation courante. Cette liste alimente ensuite le filtre `{ createdBy: { id: { $in: string[] } } }` appliqu√© syst√©matiquement √† chaque op√©ration de Repository.

Cette approche d√©fensive garantit qu'm√™me si un utilisateur tente d'acc√©der √† un workout qui ne lui appartient pas (que ce soit par erreur de requ√™te, bug dans l'interface, ou tentative malveillante), la requ√™te retournera `null` car les conditions de filtrage l'excluront automatiquement. La s√©curit√© est ainsi assur√©e m√™me en cas de faille potentielle dans la couche de pr√©sentation ou d'authentification, cr√©ant une d√©fense en profondeur au niveau de la persistance des donn√©es.

#### G√©n√©ration automatique des requ√™tes

MikroORM s'appuie sur Knex.js, une biblioth√®que JavaScript de construction de requ√™tes SQL (query builder), pour transformer les op√©rations TypeScript en requ√™tes PostgreSQL. Cette couche d'abstraction permet √† MikroORM de g√©n√©rer automatiquement les requ√™tes optimis√©es sans que je doive les √©crire manuellement.

Lorsque j'utilise l'option `populate` pour charger les relations d'une entit√©, MikroORM g√©n√®re automatiquement toutes les jointures n√©cessaires en une seule requ√™te plut√¥t que d'ex√©cuter une requ√™te par relation. Cette optimisation √©vite le "probl√®me N+1" : au lieu de faire 1 requ√™te pour r√©cup√©rer un workout puis N requ√™tes suppl√©mentaires pour r√©cup√©rer chacun de ses √©l√©ments, MikroORM g√©n√®re une seule requ√™te avec toutes les jointures LEFT n√©cessaires.

Par exemple, pour r√©cup√©rer un workout avec tous ses √©l√©ments et leurs relations, une approche na√Øve n√©cessiterait potentiellement des dizaines de requ√™tes s√©par√©es. Avec le populate, MikroORM g√©n√®re une seule requ√™te avec toutes les jointures, r√©duisant drastiquement les aller-retours avec la base de donn√©es et am√©liorant les performances.

Cette couche Infrastructure isole compl√®tement la logique m√©tier des d√©tails techniques de persistence. Je peux changer d'ORM (vers Prisma, TypeORM) ou de base de donn√©es (vers MySQL, SQLite) sans impacter les Use Cases, seuls les Repositories devront √™tre r√©impl√©ment√©s en respectant les m√™mes interfaces.

#### B√©n√©fices de l'architecture en couches

Cette s√©paration en couches r√©sout plusieurs probl√®mes que j'ai identifi√©s dans des architectures plus simples. La testabilit√© s'am√©liore consid√©rablement car chaque couche peut √™tre test√©e ind√©pendamment en simulant ses d√©pendances : les Use Cases se testent sans connexion √† la base de donn√©es, les Controllers sans logique m√©tier complexe. 

L'√©volutivit√© devient naturelle puisque la modification d'une couche n'impacte pas les autres. Je peux faire √©voluer le mod√®le de donn√©es sans toucher √† l'API REST, ou changer l'interface d'exposition sans modifier la logique m√©tier. Cette ind√©pendance facilite grandement la maintenance et l'ajout de nouvelles fonctionnalit√©s.

La r√©utilisabilit√© constitue √©galement un avantage important. Les Use Cases peuvent √™tre r√©utilis√©s par diff√©rentes interfaces d'exposition (API REST, GraphQL, interface en ligne de commande) sans duplication de code. Cette architecture modulaire me permet d'envisager sereinement l'√©volution technique de DropIt selon les besoins futurs.

### Flux de donn√©es

Pour illustrer concr√®tement cette architecture, voici le trajet d'une requ√™te simple de r√©cup√©ration d'un workout :

```mermaid
sequenceDiagram
    participant Client as üåê Client Web
    participant Controller as üéõÔ∏è WorkoutController
    participant UseCase as üìã WorkoutUseCases
    participant Repo as üì¶ MikroWorkoutRepository
    participant ORM as üîÑ MikroORM
    participant DB as üíæ PostgreSQL
    participant Mapper as üîÑ WorkoutMapper
    participant Presenter as üì§ WorkoutPresenter

    Client->>Controller: GET /api/workouts/123
    Controller->>UseCase: getWorkoutWithDetails(id, orgId, userId)
    
    UseCase->>UseCase: V√©rification permissions coach
    UseCase->>Repo: getOneWithDetails(id, filterConditions)
    
    Repo->>ORM: em.findOne(Workout, conditions, populate)
    ORM->>DB: SELECT avec LEFT JOIN (auto-g√©n√©r√©e)
    DB-->>ORM: R√©sultat SQL brut
    ORM-->>Repo: Entit√© Workout hydrat√©e
    
    Repo-->>UseCase: Workout avec relations
    UseCase->>Mapper: WorkoutMapper.toDto(workout)
    Mapper-->>UseCase: WorkoutDto typ√©
    
    UseCase->>Presenter: WorkoutPresenter.presentOne(dto)
    Presenter-->>UseCase: Response format√©e
    UseCase-->>Controller: Response
    Controller-->>Client: HTTP 200 + JSON
```

Ce diagramme illustre comment chaque couche a sa responsabilit√© sp√©cifique : le Controller g√®re le protocole HTTP, le UseCase orchestre la logique m√©tier et les permissions, le Repository abstrait l'acc√®s aux donn√©es, et le Mapper/Presenter formatent les donn√©es pour le client.

Cette approche technique avec MikroORM repr√©sente un choix d√©lib√©r√© face aux alternatives que je ma√Ætrise. L'√©criture manuelle de requ√™tes SQL m'aurait permis un contr√¥le fin sur les performances et l'optimisation, mais au prix d'une complexit√© de maintenance importante. Les requ√™tes avec jointures multiples n√©cessitent une gestion minutieuse du mapping vers les objets TypeScript et une logique de regroupement complexe pour reconstruire les hi√©rarchies relationnelles.

En optant pour MikroORM, j'ai privil√©gi√© la productivit√© de d√©veloppement et la s√©curit√© du typage strict. L'ORM √©limine le mapping manuel, d√©tecte les erreurs de relations √† la compilation, et g√©n√®re automatiquement les jointures optimis√©es. Cette approche reste flexible : pour des cas particuliers n√©cessitant des optimisations sp√©cifiques, l'EntityManager permet toujours d'√©crire des requ√™tes SQL manuelles quand le populate atteint ses limites.

## Pattern Unit of Work et gestion transactionnelle

### Le pattern Unit of Work

Le pattern Unit of Work consiste √† maintenir une liste de tous les objets modifi√©s pendant une transaction et √† coordonner leur √©criture en base de donn√©es en une seule fois. Plut√¥t que de sauvegarder chaque modification imm√©diatement, ce pattern accumule les changements en m√©moire puis les applique tous ensemble lors d'un "flush".

MikroORM impl√©mente nativement ce pattern : lorsque je modifie une entit√© charg√©e, elle est automatiquement marqu√©e comme "dirty" sans d√©clencher imm√©diatement une requ√™te SQL. C'est seulement lors de l'appel √† `flush()` que toutes les modifications sont synchronis√©es avec la base de donn√©es dans l'ordre appropri√©.

### Transactions et propri√©t√©s ACID

Les transactions garantissent les propri√©t√©s ACID (Atomicity, Consistency, Isolation, Durability) essentielles pour l'int√©grit√© des donn√©es :

- **Atomicit√©** : Soit toutes les op√©rations r√©ussissent, soit aucune n'est appliqu√©e
- **Coh√©rence** : Les contraintes de base de donn√©es sont respect√©es √† la fin de la transaction
- **Isolation** : Les transactions concurrentes n'interf√®rent pas entre elles
- **Durabilit√©** : Une fois valid√©e, la transaction persiste m√™me en cas de panne syst√®me

Dans le contexte de DropIt, cela signifie qu'un workout ne peut pas √™tre cr√©√© avec des √©l√©ments orphelins, ou qu'un athl√®te ne peut pas √™tre supprim√© s'il participe encore √† des sessions d'entra√Ænement.

### Fonctionnement automatique avec NestJS

MikroORM s'int√®gre avec le syst√®me d'intercepteurs de NestJS pour fournir automatiquement une transaction par requ√™te HTTP. Techniquement, l'intercepteur `RequestContext` de MikroORM encapsule chaque requ√™te HTTP entrante dans un contexte transactionnel : il cr√©e automatiquement un `EntityManager` avec une transaction ouverte, l'associe au thread de traitement de la requ√™te, puis commit automatiquement si tout se passe bien ou rollback en cas d'erreur :

```typescript
async save(workout: Workout): Promise<Workout> {
  await this.em.persistAndFlush(workout); // Persiste et flush dans la transaction courante
  return workout;
}
```

L'`EntityManager` suit automatiquement les modifications apport√©es aux entit√©s charg√©es et g√©n√®re les requ√™tes SQL optimales lors du flush. Cette approche r√©duit le nombre d'aller-retours avec la base de donn√©es et garantit la coh√©rence transactionnelle.


### Gestion des suppressions en cascade

La suppression d'entit√©s avec des relations n√©cessite une gestion particuli√®re pour respecter l'int√©grit√© r√©f√©rentielle. Dans DropIt, un workout poss√®de des √©l√©ments li√©s via une cl√© √©trang√®re : supprimer le workout sans g√©rer ces √©l√©ments violerait les contraintes de base de donn√©es.

MikroORM propose plusieurs strat√©gies pour g√©rer ces suppressions. J'ai opt√© pour une approche explicite qui me donne le contr√¥le total sur l'ordre des op√©rations :

```typescript
async remove(id: string, coachFilterConditions: CoachFilterConditions): Promise<void> {
  const workoutToDelete = await this.em.findOne(
    Workout,
    { id, $or: coachFilterConditions.$or },
    { populate: ['elements'] }
  );
  
  if (!workoutToDelete) {
    return;
  }

  // Suppression explicite des √©l√©ments pour respecter les contraintes
  const elements = workoutToDelete.elements.getItems();
  for (const element of elements) {
    this.em.remove(element);
  }

  await this.em.removeAndFlush(workoutToDelete);
}
```

Cette gestion manuelle me permet d'√©viter les contraintes CASCADE au niveau SQL ou les d√©corateurs `onDelete: 'cascade'` de MikroORM. Sans cette approche, PostgreSQL rejetterait la suppression du workout avec une erreur de contrainte de cl√© √©trang√®re, puisque des √©l√©ments y font encore r√©f√©rence.

L'alternative serait de d√©finir `onDelete: 'cascade'` sur la relation `@OneToMany`, ce qui d√©l√©guerait la suppression en cascade √† MikroORM. Cependant, la suppression manuelle me donne plus de contr√¥le sur le processus : je peux facilement ajouter des logs pour tracer les suppressions, valider des r√®gles m√©tier avant chaque suppression, ou m√™me impl√©menter une suppression "soft" en marquant les entit√©s comme supprim√©es sans les effacer physiquement.

Cette flexibilit√© s'av√®re particuli√®rement utile dans un contexte professionnel o√π les exigences de tra√ßabilit√© et d'audit sont importantes pour la gestion des donn√©es sportives.

## Configuration et optimisations

### Configuration MikroORM adapt√©e aux environnements

La configuration centralis√©e dans `mikro-orm.config.ts` s'adapte selon l'environnement d'ex√©cution :

```typescript
export function createMikroOrmOptions(options?: CreateMikroOrmOptions) {
  const { isTest, ...restOptions } = options ?? {};
  const isTestEnvironment = isTest || config.env === 'test';

  return defineConfig({
    entities: ['./dist/**/*.entity.js'],
    entitiesTs: ['./src/**/*.entity.ts'],
    dbName: config.database.name,
    host: config.database.host,
    port: config.database.port,
    user: config.database.user,
    password: config.database.password,
    metadataProvider: TsMorphMetadataProvider,
    forceUtcTimezone: true,
    extensions: [SeedManager, Migrator],
    debug: config.env === 'development', // Logs SQL uniquement en d√©veloppement
    allowGlobalContext: isTestEnvironment,
  });
}
```

Cette configuration r√©v√®le plusieurs optimisations importantes selon l'environnement :

**D√©couverte automatique des entit√©s** : La configuration `entities` et `entitiesTs` permet √† MikroORM de d√©couvrir automatiquement toutes les classes annot√©es `@Entity()` via l'analyse des patterns de fichiers. En d√©veloppement, MikroORM utilise `entitiesTs` pour analyser directement les fichiers TypeScript, tandis qu'en production, il se base sur `entities` pointant vers les fichiers JavaScript compil√©s.

**Analyse statique performante** : Le `TsMorphMetadataProvider` analyse le code TypeScript √† la compilation plut√¥t qu'au runtime, √©liminant le besoin de d√©corateurs reflect-metadata co√ªteux. Cette approche r√©duit significativement le temps de d√©marrage de l'application et son empreinte m√©moire en production.

**Coh√©rence temporelle** : `forceUtcTimezone: true` garantit que toutes les dates sont stock√©es et manipul√©es en UTC, √©vitant les probl√®mes de fuseaux horaires lors des d√©ploiements multi-r√©gions ou des changements d'heure saisonniers.

### Gestion des migrations en production

La strat√©gie de migration adopt√©e privil√©gie la s√©curit√© et la tra√ßabilit√© en environnement de production :

```typescript
migrations: {
  path: './dist/modules/db/migrations',
  pathTs: './src/modules/db/migrations',
  allOrNothing: true, // Transactions atomiques
  disableForeignKeys: false, // Pr√©servation de l'int√©grit√©
},
```

**G√©n√©ration automatique en d√©veloppement** : Le processus `npm run db:migration:create` g√©n√®re automatiquement les fichiers de migration en analysant les diff√©rences entre les entit√©s TypeScript et le sch√©ma de base de donn√©es actuel. Cette automatisation √©limine les erreurs humaines dans la cr√©ation des scripts de migration.

**Application atomique** : Le param√®tre `allOrNothing: true` encapsule l'application de toutes les migrations en attente dans une transaction unique. Si une migration √©choue, toutes les modifications sont annul√©es, garantissant que la base de donn√©es ne reste jamais dans un √©tat incoh√©rent.

**Pr√©servation des contraintes** : `disableForeignKeys: false` maintient l'int√©grit√© r√©f√©rentielle pendant les migrations. Cette approche plus s√ªre peut n√©cessiter un ordre sp√©cifique dans certaines migrations complexes, mais elle pr√©vient toute corruption de donn√©es.

**Tra√ßabilit√© compl√®te** : Chaque migration appliqu√©e est enregistr√©e dans une table syst√®me, permettant de conna√Ætre l'√©tat exact du sch√©ma √† tout moment. Cette tra√ßabilit√© s'av√®re cruciale lors des d√©ploiements en production pour valider l'√©tat de la base de donn√©es.

### Strat√©gie diff√©renci√©e selon l'environnement

La gestion du sch√©ma de base de donn√©es suit une strat√©gie adapt√©e aux contraintes de chaque environnement.

En d√©veloppement, j'ai privil√©gi√© une approche de reconstruction compl√®te via les seeders. Cette m√©thode permet de tester rapidement les modifications de sch√©ma en supprimant et recr√©ant toutes les tables avec des donn√©es coh√©rentes. Cette flexibilit√© s'av√®re particuli√®rement utile lors des phases d'it√©ration rapide sur le mod√®le de donn√©es.

En production, cette approche n'est √©videmment pas envisageable car elle d√©truirait toutes les donn√©es utilisateur. Le syst√®me de migrations devient alors indispensable pour faire √©voluer le sch√©ma tout en pr√©servant l'int√©grit√© et la continuit√© des donn√©es des clubs et de leurs athl√®tes. 

Les migrations g√©n√©r√©es automatiquement par MikroORM peuvent √™tre v√©rifi√©es avant application, conservant ainsi le contr√¥le sur les modifications appliqu√©es :

```typescript
import { Migration } from '@mikro-orm/migrations';

export class Migration20240115000000 extends Migration {

  async up(): Promise<void> {
    this.addSql('alter table "workout" add column "difficulty_level" int null;');
    this.addSql('alter table "workout" add constraint "workout_difficulty_level_check" check ("difficulty_level" >= 1 and "difficulty_level" <= 5);');
  }

  async down(): Promise<void> {
    this.addSql('alter table "workout" drop constraint "workout_difficulty_level_check";');
    this.addSql('alter table "workout" drop column "difficulty_level";');
  }

}
```

Ce script g√©n√©r√© automatiquement r√©v√®le plusieurs aspects importants : la m√©thode `up()` applique les modifications (ajout de colonne et contrainte), tandis que `down()` permet un rollback propre si n√©cessaire. Cette transparence me permet de valider chaque modification SQL avant de l'appliquer en production, combinant l'automatisation avec le contr√¥le manuel.

## Seeders et donn√©es de test

Pour faciliter le d√©veloppement et les tests, j'ai impl√©ment√© un syst√®me de seeders qui peuple la base avec des donn√©es coh√©rentes. Ces seeders servent un double objectif : fournir un environnement de d√©veloppement reproductible et cr√©er un catalogue commun d'exercices et de techniques d'halt√©rophilie accessible √† tous les clubs :

```typescript
export async function seedComplexes(em: EntityManager): Promise<Complex[]> {
  const exercisesMap = await seedExercises(em); // D√©pendance des exercices

  const complexCategories = [
    { name: 'Arrach√©', description: "Exercices focalis√©s sur la technique de l'arrach√©" },
    { name: '√âpaul√©', description: "Exercices focalis√©s sur la technique de l'√©paul√©-jet√©" },
    { name: 'Renforcement', description: 'Exercices de musculation sp√©cifiques' },
  ];

  // Cr√©ation des cat√©gories
  const complexCategoriesMap: Record<string, ComplexCategory> = {};
  for (const complexCategory of complexCategories) {
    const categoryToCreate = new ComplexCategory();
    categoryToCreate.name = complexCategory.name;
    categoryToCreate.createdBy = null;
    await em.persistAndFlush(categoryToCreate);
    complexCategoriesMap[complexCategory.name] = categoryToCreate;
  }

  // Cr√©ation des complexes avec leurs exercices
  const complexesToCreate = [
    {
      category: 'Arrach√©',
      description: "Focus sur la technique de l'arrach√©",
      exercises: [
        { name: 'Arrach√© Debout', reps: 3 },
        { name: 'Tirage Nuque', reps: 5 },
        { name: 'Squat Clavicule', reps: 2 },
      ],
    },
    // Autres complexes...
  ];

  const complexesCreated: Complex[] = [];
  for (const complexData of complexesToCreate) {
    const complex = new Complex();
    complex.description = complexData.description;
    complex.complexCategory = complexCategoriesMap[complexData.category];
    
    await em.persistAndFlush(complex);

    // Cr√©ation des relations exercice-complexe avec ordre
    for (let i = 0; i < complexData.exercises.length; i++) {
      const exerciseData = complexData.exercises[i];
      const exerciseComplex = new ExerciseComplex();
      exerciseComplex.complex = complex;
      exerciseComplex.exercise = exercisesMap[exerciseData.name];
      exerciseComplex.order = i;
      exerciseComplex.reps = exerciseData.reps;
      
      await em.persistAndFlush(exerciseComplex);
    }
    
    complexesCreated.push(complex);
  }

  return complexesCreated;
}
```

Ce syst√®me de seeders respecte les contraintes d'int√©grit√© r√©f√©rentielle et garantit un environnement de d√©veloppement reproductible. La structure modulaire permet de r√©utiliser les donn√©es entre diff√©rents seeders tout en maintenant la coh√©rence des relations.

L'aspect particuli√®rement int√©ressant de ces seeders est leur r√¥le dans la cr√©ation de ressources partag√©es via `createdBy = null`. Ces entit√©s publiques constituent un socle commun d'exercices officiels d'halt√©rophilie (Arrach√©, √âpaul√©-Jet√©, Squat) et de complexes techniques que tous les clubs peuvent utiliser. Cette approche √©vite la duplication des donn√©es de base tout en permettant √† chaque coach de cr√©er ses propres variantes personnalis√©es qui lui appartiennent exclusivement.

## Conclusion

Cette impl√©mentation de la couche d'acc√®s aux donn√©es avec Nest.js et MikroORM m'a permis de r√©soudre les d√©fis sp√©cifiques de DropIt tout en posant les bases d'une architecture √©volutive. 

Le choix de l'approche Code First s'est av√©r√© particuli√®rement adapt√© au contexte du monorepo, permettant une coh√©rence compl√®te avec les packages partag√©s et une productivit√© optimale en d√©veloppement.

Cette architecture en couches, inspir√©e des principes hexagonaux, me donne aujourd'hui la flexibilit√© n√©cessaire pour faire √©voluer DropIt selon les besoins futurs des clubs d'halt√©rophilie. Que ce soit pour int√©grer de nouveaux types de donn√©es sportives, √©tendre les fonctionnalit√©s de planification d'entra√Ænement, ou migrer vers d'autres technologies, les fondations pos√©es r√©sisteront aux √©volutions √† venir.

La section suivante sur les [couches de pr√©sentation](/conception/presentations) explore maintenant comment ces donn√©es sont consomm√©es et pr√©sent√©es aux utilisateurs via les clients web et mobile, compl√©tant ainsi l'architecture distribu√©e de l'application.

